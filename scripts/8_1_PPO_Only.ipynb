{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b82b5-4391-47c7-9090-c9050885e01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] RL device: cpu\n",
      "[INFO] Loaded Parquet (180519, 53)\n",
      "[INFO] TME in 0.18s | train=(144415, 3), test=(36104, 3)\n",
      "[INFO] Episodes: train=144390, test=36102\n",
      "[TIME] PPO train: 3.51s\n",
      "\n",
      "=== Metrics (Accuracy, Precision, Recall, F1) ===\n",
      "           Model  Accuracy  Precision   Recall       F1\n",
      "PPO (Sequential)  0.974019   0.975195 0.974019 0.973935\n",
      "\n",
      "[OK] Saved: metrics_dataco_ppo_only.csv\n",
      "[TOTAL] 7.4s\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# DataCo on M2 (8GB): PPO (Sequential) ONLY\n",
    "# - No TensorFlow / Keras\n",
    "# - No XGBoost / classic DL heads\n",
    "# - Light features + K-fold Target Mean Encoding\n",
    "# - Route-aware sequential environment for PPO\n",
    "# ==============================================================\n",
    "\n",
    "import os, gc, time, warnings, random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- keep mac stable; cap threads ----\n",
    "os.environ[\"OMP_NUM_THREADS\"]=\"4\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"]=\"4\"\n",
    "os.environ[\"MKL_NUM_THREADS\"]=\"4\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"]=\"4\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"]=\"4\"\n",
    "\n",
    "import numpy as np; np.random.seed(42)\n",
    "import pandas as pd; random.seed(42)\n",
    "\n",
    "# ================= PyTorch / SB3 (MPS for PPO) =================\n",
    "import torch\n",
    "rl_device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"[INFO] RL device:\", rl_device)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# ================= A) CSV -> Parquet (first run only) =================\n",
    "# >>>> CHANGE THIS PATH <<<<\n",
    "# Portable loader for DataCoSupplyChainDataset.csv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "FILENAME = \"DataCoSupplyChainDataset.csv\"\n",
    "\n",
    "def resolve_data_path(filename=FILENAME):\n",
    "    \"\"\"\n",
    "    Find the CSV in common repo locations or via the SCAI_DATA env var.\n",
    "    Works when running from repo root OR from scripts/.\n",
    "    \"\"\"\n",
    "    # 1) Optional override via environment variable\n",
    "    env_dir = os.getenv(\"SCAI_DATA\")\n",
    "    if env_dir:\n",
    "        p = Path(env_dir).expanduser() / filename\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    # 2) Try common relative locations\n",
    "    cwd = Path.cwd()\n",
    "    candidates = [\n",
    "        cwd / filename,                   # .\n",
    "        cwd / \"data\" / filename,          # ./data\n",
    "        cwd / \"Data\" / filename,          # ./Data\n",
    "        cwd.parent / filename,            # ..\n",
    "        cwd.parent / \"data\" / filename,   # ../data\n",
    "        cwd.parent / \"Data\" / filename,   # ../Data\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    tried = \"\\n\".join(str(p) for p in candidates)\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find {filename}. Tried:\\n{tried}\\nCWD={cwd}\\n\"\n",
    "        \"Tip: place the file under your repo 'data/' or set SCAI_DATA to its folder.\"\n",
    "    )\n",
    "\n",
    "file_path = resolve_data_path()\n",
    "print(f\"[info] Using data at: {file_path}\")\n",
    "\n",
    "# Load CSV (low_memory=False avoids dtype issues on large files)\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "print(df.shape, \"rows x cols\")\n",
    "\n",
    "parquet_path = os.path.splitext(file_path)[0] + \".parquet\"\n",
    "\n",
    "t0_all = time.time()\n",
    "if not os.path.exists(parquet_path):\n",
    "    print(\"[INFO] Converting CSV -> Parquet (first run only)...\")\n",
    "    chunks = []\n",
    "    for ch in pd.read_csv(file_path, engine=\"python\", encoding=\"latin-1\",\n",
    "                          on_bad_lines=\"skip\", chunksize=20000):\n",
    "        chunks.append(ch)\n",
    "    df_all = pd.concat(chunks, ignore_index=True)\n",
    "    df_all.to_parquet(parquet_path, index=False)\n",
    "    del chunks, df_all; gc.collect()\n",
    "    print(\"[OK] Parquet saved:\", parquet_path)\n",
    "\n",
    "df = pd.read_parquet(parquet_path)\n",
    "print(\"[INFO] Loaded Parquet\", df.shape)\n",
    "\n",
    "# ================= B) Column mapping =================\n",
    "def first_col(d: pd.DataFrame, names):\n",
    "    for n in names:\n",
    "        if n in d.columns: return n\n",
    "    return None\n",
    "\n",
    "col_real  = first_col(df, [\"Days for shipping (real)\", \"Days for shipping (real)_\", \"TPT\"])\n",
    "col_sched = first_col(df, [\"Days for shipment (scheduled)\", \"Days for shipment (scheduled)_\"])\n",
    "col_y     = first_col(df, [\"Late_delivery_risk\", \"Is_Late\"])\n",
    "col_qty   = first_col(df, [\"Order Item Quantity\", \"Unit quantity\", \"Quantity\"])\n",
    "col_org   = first_col(df, [\"Order City\", \"Order Region\", \"Order Country\"])\n",
    "col_dst   = first_col(df, [\"Customer City\", \"Customer Region\", \"Customer Country\"])\n",
    "col_car   = first_col(df, [\"Shipping Mode\", \"Carrier\", \"Shipment Mode\", \"Ship Mode\"])\n",
    "col_date  = first_col(df, [\"Order Date (DateOrders)\", \"Order Date\", \"DateOrders\", \"Date\"])\n",
    "\n",
    "# Build target if missing\n",
    "if col_y is None:\n",
    "    if (col_real is None) or (col_sched is None):\n",
    "        raise ValueError(\"Need Late_delivery_risk or both real/scheduled days.\")\n",
    "    df[\"__y__\"] = (pd.to_numeric(df[col_real], errors=\"coerce\") >\n",
    "                   pd.to_numeric(df[col_sched], errors=\"coerce\")).astype(int)\n",
    "    col_y = \"__y__\"\n",
    "\n",
    "# ================= C) Minimal numeric + light categoricals =================\n",
    "keep_cols = [c for c in [col_qty, col_real, col_sched, col_y, col_org, col_dst, col_car, col_date] if c is not None]\n",
    "df = df[keep_cols].copy()\n",
    "\n",
    "# Cast numerics\n",
    "for c in [col_qty, col_real, col_sched]:\n",
    "    if c is not None:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Drop core NA\n",
    "df = df.dropna(subset=[col_y, col_real, col_sched, col_qty])\n",
    "\n",
    "# Target\n",
    "y_all = df[col_y].astype(int).to_numpy()\n",
    "\n",
    "# Numeric features (compact, strong)\n",
    "num_df = pd.DataFrame(index=df.index)\n",
    "num_df[\"qty\"]          = df[col_qty]\n",
    "num_df[\"tpt_real\"]     = df[col_real]\n",
    "num_df[\"tpt_sched\"]    = df[col_sched]\n",
    "num_df[\"lead_dev\"]     = num_df[\"tpt_real\"] - num_df[\"tpt_sched\"]\n",
    "num_df[\"tpt_per_unit\"] = num_df[\"tpt_real\"] / np.clip(num_df[\"qty\"].replace(0, np.nan), 1, None)\n",
    "num_df[\"log_qty\"]      = np.log1p(np.clip(num_df[\"qty\"], 1, None))\n",
    "\n",
    "# Light categoricals (no one-hot explosion)\n",
    "cat_df = pd.DataFrame(index=df.index)\n",
    "cat_df[\"org\"] = df[col_org].astype(str) if col_org else \"NA\"\n",
    "cat_df[\"dst\"] = df[col_dst].astype(str) if col_dst else \"NA\"\n",
    "cat_df[\"car\"] = df[col_car].astype(str) if col_car else \"NA\"\n",
    "\n",
    "# ================= D) Split first (avoid leakage), then target-mean encode =================\n",
    "X_num_all = num_df.to_numpy(dtype=\"float32\")\n",
    "X_cat_all = cat_df.copy()\n",
    "\n",
    "X_cat_tr, X_cat_te, X_num_tr, X_num_te, y_tr, y_te, idx_tr, idx_te = train_test_split(\n",
    "    X_cat_all, X_num_all, y_all, df.index.values,\n",
    "    test_size=0.2, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "def kfold_tme(X_cat: pd.DataFrame, y: np.ndarray, n_splits=5, alpha=10.0, seed=42):\n",
    "    \"\"\"K-fold target mean encoding with smoothing; returns encoded train and mapping.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    enc_maps = {col: [] for col in X_cat.columns}\n",
    "    X_enc = pd.DataFrame(index=X_cat.index, dtype=\"float32\")\n",
    "    global_mean = float(y.mean())\n",
    "\n",
    "    for col in X_cat.columns:\n",
    "        col_enc = pd.Series(index=X_cat.index, dtype=\"float32\")\n",
    "        for tr_idx, va_idx in skf.split(np.zeros(len(y)), y):\n",
    "            keys_tr = X_cat.iloc[tr_idx, X_cat.columns.get_loc(col)]\n",
    "            y_tr_f  = y[tr_idx]\n",
    "            grp = pd.DataFrame({\"k\": keys_tr.values, \"y\": y_tr_f})\n",
    "            stats = grp.groupby(\"k\")[\"y\"].agg([\"count\", \"mean\"])\n",
    "            smooth = (stats[\"count\"]*stats[\"mean\"] + alpha*global_mean) / (stats[\"count\"] + alpha)\n",
    "            keys_va = X_cat.iloc[va_idx, X_cat.columns.get_loc(col)]\n",
    "            col_enc.iloc[va_idx] = keys_va.map(smooth).fillna(global_mean).astype(\"float32\")\n",
    "        X_enc[col] = col_enc.values\n",
    "        # full-train map for test\n",
    "        grp_full = pd.DataFrame({\"k\": X_cat[col].values, \"y\": y})\n",
    "        stats_full = grp_full.groupby(\"k\")[\"y\"].agg([\"count\", \"mean\"])\n",
    "        smooth_full = (stats_full[\"count\"]*stats_full[\"mean\"] + alpha*global_mean) / (stats_full[\"count\"] + alpha)\n",
    "        enc_maps[col] = [smooth_full.to_dict(), float(global_mean)]\n",
    "    return X_enc.astype(\"float32\"), enc_maps\n",
    "\n",
    "def apply_tme(X_cat: pd.DataFrame, maps):\n",
    "    out = pd.DataFrame(index=X_cat.index, dtype=\"float32\")\n",
    "    for col in X_cat.columns:\n",
    "        m, g = maps[col]\n",
    "        out[col] = X_cat[col].map(m).fillna(g).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "t0 = time.time()\n",
    "Xtr_cat_tme, encoders = kfold_tme(X_cat_tr, y_tr, n_splits=5, alpha=10.0)\n",
    "Xte_cat_tme = apply_tme(X_cat_te, encoders)\n",
    "print(f\"[INFO] TME in {time.time()-t0:.2f}s | train={Xtr_cat_tme.shape}, test={Xte_cat_tme.shape}\")\n",
    "\n",
    "# Final tabular features\n",
    "Xtr_tab = np.hstack([X_num_tr, Xtr_cat_tme.to_numpy()])\n",
    "Xte_tab = np.hstack([X_num_te, Xte_cat_tme.to_numpy()])\n",
    "\n",
    "# Scale (fit on train, apply to test)\n",
    "scaler = StandardScaler()\n",
    "Xtr_tab_s = scaler.fit_transform(Xtr_tab).astype(\"float32\")\n",
    "Xte_tab_s = scaler.transform(Xte_tab).astype(\"float32\")\n",
    "\n",
    "# ================= E) Build route keys & time for episodes =================\n",
    "def safe_series(indexes, colname):\n",
    "    if colname and colname in df.columns:\n",
    "        return df.loc[indexes, colname].astype(str)\n",
    "    return pd.Series(\"NA\", index=indexes)\n",
    "\n",
    "def get_time_series(indexes):\n",
    "    if col_date and col_date in df.columns:\n",
    "        return pd.to_datetime(df.loc[indexes, col_date], errors=\"coerce\")\n",
    "    # fallback synthetic time\n",
    "    return pd.Series(pd.date_range(\"2000-01-01\", periods=len(indexes), freq=\"H\"), index=indexes)\n",
    "\n",
    "def build_ordering_and_routes(indices):\n",
    "    order_time = get_time_series(indices)\n",
    "    route_key = (safe_series(indices, col_org) + \" | \" +\n",
    "                 safe_series(indices, col_dst) + \" | \" +\n",
    "                 safe_series(indices, col_car)).astype(str)\n",
    "    ord_idx = np.argsort(order_time.values)\n",
    "    return ord_idx, route_key.values\n",
    "\n",
    "# Train ordering / episodes\n",
    "ord_tr, routes_tr = build_ordering_and_routes(idx_tr)\n",
    "Xtr_seq = Xtr_tab_s[ord_tr]\n",
    "ytr_seq = y_tr[ord_tr]\n",
    "routes_tr = routes_tr[ord_tr]\n",
    "\n",
    "# Test ordering / episodes\n",
    "ord_te, routes_te = build_ordering_and_routes(idx_te)\n",
    "Xte_seq = Xte_tab_s[ord_te]\n",
    "yte_seq = y_te[ord_te]\n",
    "routes_te = routes_te[ord_te]\n",
    "\n",
    "def build_episodes(routes):\n",
    "    episodes, start = [], 0\n",
    "    for i in range(1, len(routes)+1):\n",
    "        if i == len(routes) or routes[i] != routes[i-1]:\n",
    "            episodes.append(slice(start, i)); start = i\n",
    "    return episodes\n",
    "\n",
    "episodes_tr = build_episodes(routes_tr)\n",
    "episodes_te = build_episodes(routes_te)\n",
    "\n",
    "print(f\"[INFO] Episodes: train={len(episodes_tr)}, test={len(episodes_te)}\")\n",
    "\n",
    "# ================= F) PPO Environment (Sequential) =================\n",
    "class SeqEnv(gym.Env):\n",
    "    metadata = {\"render_modes\":[]}\n",
    "    def __init__(self, X, y, episodes, K=5):\n",
    "        super().__init__()\n",
    "        self.X, self.y, self.episodes, self.K = X, y.astype(int), episodes, K\n",
    "        # obs = features + 2 time-pos enc + last-K actions + FP/FN counters\n",
    "        self.obs_dim = X.shape[1] + 2 + K + 2\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.obs_dim,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self._ep = -1\n",
    "\n",
    "    def _tfeat(self, t, T):\n",
    "        pos = t/max(T-1,1)\n",
    "        return np.array([np.sin(2*np.pi*pos), np.cos(2*np.pi*pos)], dtype=np.float32)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self._ep = (self._ep + 1) % len(self.episodes)\n",
    "        sl = self.episodes[self._ep]\n",
    "        self.idx = np.arange(sl.start, sl.stop, dtype=int)\n",
    "        self.t = 0\n",
    "        self.last = np.zeros(self.K, dtype=np.float32)\n",
    "        self.fp = 0.0; self.fn = 0.0; self.prev_fn = 0.0\n",
    "        return self._obs(), {}\n",
    "\n",
    "    def _obs(self):\n",
    "        T = len(self.idx); i = self.idx[self.t]\n",
    "        return np.concatenate([self.X[i], self._tfeat(self.t, T), self.last,\n",
    "                               np.array([self.fp, self.fn], dtype=np.float32)]).astype(np.float32)\n",
    "\n",
    "    def step(self, a):\n",
    "        i = self.idx[self.t]; yv = self.y[i]\n",
    "        # Reward: FN >> FP (missed delays costly); small step cost; small bonus if FN-rate improves\n",
    "        r = 2.0 if (a==1 and yv==1) else (1.0 if (a==0 and yv==0) else (-5.0 if (yv==1 and a==0) else -2.0))\n",
    "        r -= 0.01\n",
    "        self.fn += float(yv==1 and a==0); self.fp += float(yv==0 and a==1)\n",
    "        steps = float(self.t + 1); fn_rate = self.fn/max(steps, 1.0)\n",
    "        if fn_rate < self.prev_fn: r += 0.2\n",
    "        self.prev_fn = fn_rate\n",
    "\n",
    "        self.last = np.roll(self.last, -1); self.last[-1] = float(a)\n",
    "        self.t += 1\n",
    "        done = self.t >= len(self.idx)\n",
    "        obs = np.zeros(self.observation_space.shape[0], dtype=np.float32) if done else self._obs()\n",
    "        return obs, float(r), done, False, {}\n",
    "\n",
    "# ================= G) Train PPO on TRAIN EPISODES; Evaluate on TEST =================\n",
    "PPO_STEPS = 20_000       # increase if you want better convergence\n",
    "PPO_BS    = 4096\n",
    "PPO_NST   = 1024\n",
    "\n",
    "train_env = make_vec_env(lambda: SeqEnv(Xtr_seq, ytr_seq, episodes_tr, K=5), n_envs=1)\n",
    "ppo = PPO(\n",
    "    \"MlpPolicy\", train_env, seed=42, verbose=0, device=rl_device,\n",
    "    batch_size=PPO_BS, n_steps=PPO_NST, learning_rate=3e-4\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "ppo.learn(total_timesteps=PPO_STEPS)\n",
    "print(f\"[TIME] PPO train: {time.time()-t0:.2f}s\")\n",
    "\n",
    "# Deterministic evaluation on TEST episodes\n",
    "eval_env = SeqEnv(Xte_seq, yte_seq, episodes_te, K=5)\n",
    "obs, _ = eval_env.reset(); preds = []; visited = 0\n",
    "while True:\n",
    "    a, _ = ppo.predict(obs, deterministic=True)\n",
    "    preds.append(int(a))\n",
    "    obs, _, done, _, _ = eval_env.step(a)\n",
    "    if done:\n",
    "        visited += 1\n",
    "        if visited >= len(episodes_te): break\n",
    "        obs, _ = eval_env.reset()\n",
    "preds = np.array(preds[:len(yte_seq)])\n",
    "\n",
    "# ================= H) Metrics =================\n",
    "def metric_dict(y_true, y_pred_binary):\n",
    "    ypred = np.asarray(y_pred_binary).reshape(-1).astype(int)\n",
    "    return dict(\n",
    "        Accuracy  = float(accuracy_score(y_true, ypred)),\n",
    "        Precision = float(precision_score(y_true, ypred, average=\"weighted\", zero_division=1)),\n",
    "        Recall    = float(recall_score(y_true, ypred, average=\"weighted\", zero_division=1)),\n",
    "        F1        = float(f1_score(y_true, ypred, average=\"weighted\", zero_division=1))\n",
    "    )\n",
    "\n",
    "results = {\"PPO (Sequential)\": metric_dict(yte_seq, preds)}\n",
    "final_df = pd.DataFrame(results).T.reset_index().rename(columns={\"index\":\"Model\"})\n",
    "print(\"\\n=== Metrics (Accuracy, Precision, Recall, F1) ===\")\n",
    "print(final_df.to_string(index=False))\n",
    "\n",
    "# Save\n",
    "out_path = \"metrics_dataco_ppo_only.csv\"\n",
    "final_df.to_csv(out_path, index=False)\n",
    "print(f\"\\n[OK] Saved: {out_path}\")\n",
    "print(f\"[TOTAL] {time.time()-t0_all:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53834f7-e742-42fa-8e34-5347595eadc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda3)",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
