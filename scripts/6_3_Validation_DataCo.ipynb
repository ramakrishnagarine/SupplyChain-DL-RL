{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e04d0d9-63a1-4ef2-b9ed-a0d47ef0846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] RL device: cpu\n",
      "[INFO] Loaded: (180519, 53)\n",
      "[INFO] TME: 0.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 13:47:46.950740: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n",
      "2025-08-17 13:48:51.971308: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME] PPO learn: 34.3s\n",
      "\n",
      "--- XGBoost ---\n",
      "Acc/Prec/Rec/F1 (weighted): {'Accuracy': 0.974435, 'Precision': 0.975569, 'Recall': 0.974435, 'F1': 0.974354}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9999    0.9435    0.9709     16308\n",
      "           1     0.9555    0.9999    0.9772     19796\n",
      "\n",
      "    accuracy                         0.9744     36104\n",
      "   macro avg     0.9777    0.9717    0.9740     36104\n",
      "weighted avg     0.9756    0.9744    0.9744     36104\n",
      "\n",
      "Confusion Matrix:\n",
      " [[15386   922]\n",
      " [    1 19795]]\n",
      "\n",
      "--- CNN ---\n",
      "Acc/Prec/Rec/F1 (weighted): {'Accuracy': 0.974463, 'Precision': 0.975599, 'Recall': 0.974463, 'F1': 0.974382}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9435    0.9709     16308\n",
      "           1     0.9555    1.0000    0.9772     19796\n",
      "\n",
      "    accuracy                         0.9745     36104\n",
      "   macro avg     0.9777    0.9717    0.9741     36104\n",
      "weighted avg     0.9756    0.9745    0.9744     36104\n",
      "\n",
      "Confusion Matrix:\n",
      " [[15386   922]\n",
      " [    0 19796]]\n",
      "\n",
      "--- LSTM ---\n",
      "Acc/Prec/Rec/F1 (weighted): {'Accuracy': 0.974463, 'Precision': 0.975599, 'Recall': 0.974463, 'F1': 0.974382}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9435    0.9709     16308\n",
      "           1     0.9555    1.0000    0.9772     19796\n",
      "\n",
      "    accuracy                         0.9745     36104\n",
      "   macro avg     0.9777    0.9717    0.9741     36104\n",
      "weighted avg     0.9756    0.9745    0.9744     36104\n",
      "\n",
      "Confusion Matrix:\n",
      " [[15386   922]\n",
      " [    0 19796]]\n",
      "\n",
      "--- Bi-LSTM ---\n",
      "Acc/Prec/Rec/F1 (weighted): {'Accuracy': 0.972773, 'Precision': 0.974017, 'Recall': 0.972773, 'F1': 0.972682}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9995    0.9402    0.9689     16308\n",
      "           1     0.9530    0.9996    0.9758     19796\n",
      "\n",
      "    accuracy                         0.9728     36104\n",
      "   macro avg     0.9763    0.9699    0.9724     36104\n",
      "weighted avg     0.9740    0.9728    0.9727     36104\n",
      "\n",
      "Confusion Matrix:\n",
      " [[15333   975]\n",
      " [    8 19788]]\n",
      "\n",
      "--- Stacked LSTM ---\n",
      "Acc/Prec/Rec/F1 (weighted): {'Accuracy': 0.974435, 'Precision': 0.975569, 'Recall': 0.974435, 'F1': 0.974354}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9999    0.9435    0.9709     16308\n",
      "           1     0.9555    0.9999    0.9772     19796\n",
      "\n",
      "    accuracy                         0.9744     36104\n",
      "   macro avg     0.9777    0.9717    0.9740     36104\n",
      "weighted avg     0.9756    0.9744    0.9744     36104\n",
      "\n",
      "Confusion Matrix:\n",
      " [[15386   922]\n",
      " [    1 19795]]\n",
      "\n",
      "[NOTE] PPO/Ensemble evaluated on time-sorted test sequence (same items).\n",
      "\n",
      "--- Static Ensemble (avg logits) ---\n",
      "Acc/Prec/Rec/F1 (weighted): {'Accuracy': 0.974463, 'Precision': 0.975599, 'Recall': 0.974463, 'F1': 0.974382}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9435    0.9709     16308\n",
      "           1     0.9555    1.0000    0.9772     19796\n",
      "\n",
      "    accuracy                         0.9745     36104\n",
      "   macro avg     0.9777    0.9717    0.9741     36104\n",
      "weighted avg     0.9756    0.9745    0.9744     36104\n",
      "\n",
      "Confusion Matrix:\n",
      " [[15386   922]\n",
      " [    0 19796]]\n",
      "\n",
      "--- PPO Sequential ---\n",
      "Acc/Prec/Rec/F1 (weighted): {'Accuracy': 0.974463, 'Precision': 0.975599, 'Recall': 0.974463, 'F1': 0.974382}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9435    0.9709     16308\n",
      "           1     0.9555    1.0000    0.9772     19796\n",
      "\n",
      "    accuracy                         0.9745     36104\n",
      "   macro avg     0.9777    0.9717    0.9741     36104\n",
      "weighted avg     0.9756    0.9745    0.9744     36104\n",
      "\n",
      "Confusion Matrix:\n",
      " [[15386   922]\n",
      " [    0 19796]]\n",
      "\n",
      "=== Summary (weighted) + Recall_late ===\n",
      "          Model  Accuracy  Precision   Recall       F1  Recall_late\n",
      "        XGBoost  0.974435   0.975569 0.974435 0.974354     0.999949\n",
      "            CNN  0.974463   0.975599 0.974463 0.974382     1.000000\n",
      "           LSTM  0.974463   0.975599 0.974463 0.974382     1.000000\n",
      "        Bi-LSTM  0.972773   0.974017 0.972773 0.972682     0.999596\n",
      "   Stacked LSTM  0.974435   0.975569 0.974435 0.974354     0.999949\n",
      "Static Ensemble  0.974463   0.975599 0.974463 0.974382     1.000000\n",
      " PPO Sequential  0.974463   0.975599 0.974463 0.974382     1.000000\n",
      "\n",
      "[OK] Saved: metrics_ppov3_with_recall_late.csv\n",
      "[TOTAL] 108.0s\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# DataCo: DL/XGB experts + Sequential PPO with drift-aware episodes\n",
    "# Goal: PPO adds value by maximizing late-class recall/F1 beyond static models\n",
    "# ==============================================================\n",
    "\n",
    "import os, gc, time, warnings, random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- mac stability\n",
    "os.environ[\"OMP_NUM_THREADS\"]=\"4\"; os.environ[\"OPENBLAS_NUM_THREADS\"]=\"4\"\n",
    "os.environ[\"MKL_NUM_THREADS\"]=\"4\"; os.environ[\"VECLIB_MAXIMUM_THREADS\"]=\"4\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"]=\"4\"\n",
    "\n",
    "import numpy as np; np.random.seed(42)\n",
    "import pandas as pd; random.seed(42)\n",
    "\n",
    "# ============== TensorFlow (DL heads) ==============\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import mixed_precision\n",
    "try:\n",
    "    mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "except: pass\n",
    "try:\n",
    "    for g in tf.config.list_physical_devices('GPU'):\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "except: pass\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ============== PyTorch / SB3 (PPO) ==============\n",
    "import torch\n",
    "rl_device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"[INFO] RL device:\", rl_device)\n",
    "\n",
    "# Sklearn & metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             classification_report, confusion_matrix)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Gym/SB3\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize, VecMonitor\n",
    "\n",
    "# ================= A) Load / cache parquet =================\n",
    "# >>>> CHANGE THIS PATH <<<<\n",
    "# Portable loader for DataCoSupplyChainDataset.csv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "FILENAME = \"DataCoSupplyChainDataset.csv\"\n",
    "\n",
    "def resolve_data_path(filename=FILENAME):\n",
    "    \"\"\"\n",
    "    Find the CSV in common repo locations or via the SCAI_DATA env var.\n",
    "    Works when running from repo root OR from scripts/.\n",
    "    \"\"\"\n",
    "   \n",
    "    env_dir = os.getenv(\"SCAI_DATA\")\n",
    "    if env_dir:\n",
    "        p = Path(env_dir).expanduser() / filename\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "   \n",
    "    cwd = Path.cwd()\n",
    "    candidates = [\n",
    "        cwd / filename,                   # .\n",
    "        cwd / \"data\" / filename,          # ./data\n",
    "        cwd / \"Data\" / filename,          # ./Data\n",
    "        cwd.parent / filename,            # ..\n",
    "        cwd.parent / \"data\" / filename,   # ../data\n",
    "        cwd.parent / \"Data\" / filename,   # ../Data\n",
    "        cwd.parents[1] / \"data\" / filename,  # ../../data\n",
    "        cwd.parents[1] / \"Data\" / filename,  # ../../Data\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    tried = \"\\n\".join(str(p) for p in candidates)\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find {filename}. Tried:\\n{tried}\\nCWD={cwd}\\n\"\n",
    "        \"Tip: place the file under your repo 'data/' or set SCAI_DATA to its folder.\"\n",
    "    )\n",
    "\n",
    "FILE_PATH = resolve_data_path()\n",
    "print(f\"[info] Using data at: {FILE_PATH}\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(FILE_PATH, low_memory=False)\n",
    "print(df.shape, \"rows x cols\")\n",
    "\n",
    "parquet_path = os.path.splitext(file_path)[0] + \".parquet\"\n",
    "\n",
    "t0_all = time.time()\n",
    "if not os.path.exists(parquet_path):\n",
    "    print(\"[INFO] Converting CSV -> Parquet...\")\n",
    "    chunks = []\n",
    "    for ch in pd.read_csv(file_path, engine=\"python\", encoding=\"latin-1\",\n",
    "                          on_bad_lines=\"skip\", chunksize=20000):\n",
    "        chunks.append(ch)\n",
    "    pd.concat(chunks, ignore_index=True).to_parquet(parquet_path, index=False)\n",
    "    del chunks; gc.collect()\n",
    "\n",
    "df = pd.read_parquet(parquet_path)\n",
    "print(\"[INFO] Loaded:\", df.shape)\n",
    "\n",
    "# ================= B) Column mapping =================\n",
    "def first_col(d, names):\n",
    "    for n in names:\n",
    "        if n in d.columns: return n\n",
    "    return None\n",
    "\n",
    "col_real  = first_col(df, [\"Days for shipping (real)\", \"Days for shipping (real)_\", \"TPT\"])\n",
    "col_sched = first_col(df, [\"Days for shipment (scheduled)\", \"Days for shipment (scheduled)_\"])\n",
    "col_y     = first_col(df, [\"Late_delivery_risk\", \"Is_Late\"])\n",
    "col_qty   = first_col(df, [\"Order Item Quantity\", \"Unit quantity\", \"Quantity\"])\n",
    "col_org   = first_col(df, [\"Order City\", \"Order Region\", \"Order Country\"])\n",
    "col_dst   = first_col(df, [\"Customer City\", \"Customer Region\", \"Customer Country\"])\n",
    "col_car   = first_col(df, [\"Shipping Mode\", \"Carrier\", \"Shipment Mode\", \"Ship Mode\"])\n",
    "col_date  = first_col(df, [\"Order Date (DateOrders)\", \"Order Date\", \"DateOrders\", \"Date\"])\n",
    "col_val   = first_col(df, [\"Order Item Total\", \"Sales\"])\n",
    "\n",
    "# target if missing\n",
    "if col_y is None:\n",
    "    if (col_real is None) or (col_sched is None):\n",
    "        raise ValueError(\"Need Late_delivery_risk or both real/scheduled days.\")\n",
    "    df[\"__y__\"] = (pd.to_numeric(df[col_real], errors=\"coerce\") >\n",
    "                   pd.to_numeric(df[col_sched], errors=\"coerce\")).astype(int)\n",
    "    col_y = \"__y__\"\n",
    "\n",
    "keep = [c for c in [col_qty, col_real, col_sched, col_y, col_org, col_dst, col_car, col_date, col_val] if c]\n",
    "df = df[keep].copy()\n",
    "\n",
    "# numerics\n",
    "for c in [col_qty, col_real, col_sched]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df = df.dropna(subset=[col_y, col_real, col_sched, col_qty])\n",
    "\n",
    "y_all = df[col_y].astype(int).to_numpy()\n",
    "\n",
    "# ================= C) Base features (compact) =================\n",
    "num_df = pd.DataFrame(index=df.index)\n",
    "num_df[\"qty\"]         = df[col_qty]\n",
    "num_df[\"tpt_real\"]    = df[col_real]\n",
    "num_df[\"tpt_sched\"]   = df[col_sched]\n",
    "num_df[\"lead_dev\"]    = num_df[\"tpt_real\"] - num_df[\"tpt_sched\"]\n",
    "num_df[\"tpt_per_unit\"]= num_df[\"tpt_real\"] / np.clip(num_df[\"qty\"].replace(0, np.nan), 1, None)\n",
    "num_df[\"log_qty\"]     = np.log1p(np.clip(num_df[\"qty\"], 1, None))\n",
    "\n",
    "cat_df = pd.DataFrame(index=df.index)\n",
    "cat_df[\"org\"] = df[col_org].astype(str) if col_org else \"NA\"\n",
    "cat_df[\"dst\"] = df[col_dst].astype(str) if col_dst else \"NA\"\n",
    "cat_df[\"car\"] = df[col_car].astype(str) if col_car else \"NA\"\n",
    "\n",
    "# ================= D) Split then TME (no leakage) =================\n",
    "X_num_all = num_df.to_numpy(dtype=\"float32\")\n",
    "X_cat_all = cat_df.copy()\n",
    "\n",
    "X_cat_tr, X_cat_te, X_num_tr, X_num_te, y_tr, y_te = train_test_split(\n",
    "    X_cat_all, X_num_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "def kfold_tme(X_cat: pd.DataFrame, y: np.ndarray, n_splits=5, alpha=10.0, seed=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    enc_maps = {col: [] for col in X_cat.columns}\n",
    "    X_enc = pd.DataFrame(index=X_cat.index, dtype=\"float32\")\n",
    "    global_mean = float(y.mean())\n",
    "    for col in X_cat.columns:\n",
    "        col_enc = pd.Series(index=X_cat.index, dtype=\"float32\")\n",
    "        for tr_idx, va_idx in skf.split(np.zeros(len(y)), y):\n",
    "            keys_tr = X_cat.iloc[tr_idx, X_cat.columns.get_loc(col)]\n",
    "            y_tr_f  = y[tr_idx]\n",
    "            stats = pd.DataFrame({\"k\": keys_tr.values, \"y\": y_tr_f}).groupby(\"k\")[\"y\"].agg([\"count\",\"mean\"])\n",
    "            smooth = (stats[\"count\"]*stats[\"mean\"] + alpha*global_mean) / (stats[\"count\"] + alpha)\n",
    "            keys_va = X_cat.iloc[va_idx, X_cat.columns.get_loc(col)]\n",
    "            col_enc.iloc[va_idx] = keys_va.map(smooth).fillna(global_mean).astype(\"float32\")\n",
    "        X_enc[col] = col_enc.values\n",
    "        stats_full = pd.DataFrame({\"k\": X_cat[col].values, \"y\": y}).groupby(\"k\")[\"y\"].agg([\"count\",\"mean\"])\n",
    "        smooth_full = (stats_full[\"count\"]*stats_full[\"mean\"] + alpha*global_mean) / (stats_full[\"count\"] + alpha)\n",
    "        enc_maps[col] = [smooth_full.to_dict(), float(global_mean)]\n",
    "    return X_enc.astype(\"float32\"), enc_maps\n",
    "\n",
    "def apply_tme(X_cat: pd.DataFrame, maps):\n",
    "    out = pd.DataFrame(index=X_cat.index, dtype=\"float32\")\n",
    "    for col in X_cat.columns:\n",
    "        m, g = maps[col]\n",
    "        out[col] = X_cat[col].map(m).fillna(g).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "t0 = time.time()\n",
    "Xtr_cat_tme, encoders = kfold_tme(X_cat_tr, y_tr, n_splits=5, alpha=10.0)\n",
    "Xte_cat_tme = apply_tme(X_cat_te, encoders)\n",
    "print(f\"[INFO] TME: {time.time()-t0:.2f}s\")\n",
    "\n",
    "Xtr_tab = np.hstack([X_num_tr, Xtr_cat_tme.to_numpy()])\n",
    "Xte_tab = np.hstack([X_num_te, Xte_cat_tme.to_numpy()])\n",
    "\n",
    "scaler_tab = StandardScaler()\n",
    "Xtr_tab_s = scaler_tab.fit_transform(Xtr_tab).astype(\"float32\")\n",
    "Xte_tab_s = scaler_tab.transform(Xte_tab).astype(\"float32\")\n",
    "\n",
    "Xtr_3d = Xtr_tab_s.reshape(-1, Xtr_tab_s.shape[1], 1)\n",
    "Xte_3d = Xte_tab_s.reshape(-1, Xte_tab_s.shape[1], 1)\n",
    "\n",
    "def ds(X, y, batch=1024, shuffle=True):\n",
    "    d = tf.data.Dataset.from_tensor_slices((X, y.astype(np.int32)))\n",
    "    if shuffle: d = d.shuffle(65536, seed=42, reshuffle_each_iteration=True)\n",
    "    return d.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "ds_tr, ds_te = ds(Xtr_3d, y_tr), ds(Xte_3d, y_te, shuffle=False)\n",
    "\n",
    "# ================= E) Metrics helpers =================\n",
    "def metric_dict(y_true, y_hat_or_proba):\n",
    "    arr = np.asarray(y_hat_or_proba).reshape(-1)\n",
    "    ypred = arr.astype(int) if set(np.unique(arr)) <= {0,1} else (arr >= 0.5).astype(int)\n",
    "    return dict(\n",
    "        Accuracy  = float(accuracy_score(y_true, ypred)),\n",
    "        Precision = float(precision_score(y_true, ypred, average=\"weighted\", zero_division=1)),\n",
    "        Recall    = float(recall_score(y_true, ypred, average=\"weighted\", zero_division=1)),\n",
    "        F1        = float(f1_score(y_true, ypred, average=\"weighted\", zero_division=1))\n",
    "    )\n",
    "\n",
    "# ================= F) Experts =================\n",
    "pos_ratio = float(np.mean(y_tr))\n",
    "scale_pos_weight = float((1 - pos_ratio) / max(pos_ratio, 1e-6))\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=600, learning_rate=0.06, max_depth=6,\n",
    "    subsample=0.9, colsample_bytree=0.9,\n",
    "    tree_method=\"hist\", eval_metric=\"logloss\",\n",
    "    random_state=42, n_jobs=4,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "xgb.fit(Xtr_tab, y_tr); proba_xgb = xgb.predict_proba(Xte_tab)[:,1]\n",
    "\n",
    "def build_dl(kind, d):\n",
    "    m = Sequential([Input(shape=(d,1))])\n",
    "    if kind==\"CNN\":\n",
    "        m.add(Conv1D(64, 3, activation=\"relu\")); m.add(Flatten())\n",
    "    elif kind==\"LSTM\":\n",
    "        m.add(LSTM(64))\n",
    "    elif kind==\"Stacked LSTM\":\n",
    "        m.add(LSTM(64, return_sequences=True)); m.add(LSTM(32))\n",
    "    elif kind==\"Bi-LSTM\":\n",
    "        m.add(Bidirectional(LSTM(64)))\n",
    "    m.add(Dense(1, activation=\"sigmoid\", dtype=\"float32\"))\n",
    "    m.compile(optimizer=Adam(1e-3), loss=\"binary_crossentropy\")\n",
    "    return m\n",
    "\n",
    "DL_EPOCHS=6\n",
    "dl_outs={}\n",
    "for kind in [\"CNN\",\"LSTM\",\"Bi-LSTM\",\"Stacked LSTM\"]:\n",
    "    mdl=build_dl(kind, Xtr_3d.shape[1])\n",
    "    mdl.fit(ds_tr, epochs=DL_EPOCHS, verbose=0)\n",
    "    dl_outs[kind]=mdl.predict(ds_te, verbose=0).reshape(-1)\n",
    "    del mdl; gc.collect()\n",
    "\n",
    "# ============== Build stacked logits for PPO ==============\n",
    "def to_logit(p, eps=1e-6):\n",
    "    p = np.clip(p, eps, 1-eps)\n",
    "    return np.log(p) - np.log(1-p)\n",
    "\n",
    "stack_logits = np.vstack([\n",
    "    to_logit(proba_xgb),\n",
    "    to_logit(dl_outs[\"CNN\"]),\n",
    "    to_logit(dl_outs[\"LSTM\"]),\n",
    "    to_logit(dl_outs[\"Bi-LSTM\"]),\n",
    "    to_logit(dl_outs[\"Stacked LSTM\"])\n",
    "]).T.astype(\"float32\")\n",
    "\n",
    "scaler_stack = StandardScaler()\n",
    "stack5 = scaler_stack.fit_transform(stack_logits).astype(\"float32\")\n",
    "\n",
    "# ============== Build episodes with temporal drift ==============\n",
    "idx_te = X_cat_te.index\n",
    "if col_date and col_date in df.columns:\n",
    "    tseries = pd.to_datetime(df.loc[idx_te, col_date], errors=\"coerce\")\n",
    "else:\n",
    "    tseries = pd.Series(pd.date_range(\"2000-01-01\", periods=len(idx_te), freq=\"H\"), index=idx_te)\n",
    "\n",
    "def safe_series(colname):\n",
    "    if colname and colname in df.columns:\n",
    "        return df.loc[idx_te, colname].astype(str)\n",
    "    return pd.Series(\"NA\", index=idx_te)\n",
    "\n",
    "route = safe_series(col_org) + \"|\" + safe_series(col_dst) + \"|\" + safe_series(col_car)\n",
    "month = tseries.dt.to_period(\"M\").astype(str)\n",
    "\n",
    "# sort by time, then group by (route, month) to surface drift\n",
    "ord_idx = np.argsort(tseries.values)\n",
    "y_seq  = y_te[ord_idx]\n",
    "stack5 = stack5[ord_idx]\n",
    "route  = route.iloc[ord_idx].values\n",
    "month  = month.iloc[ord_idx].values\n",
    "\n",
    "# engineered sequential signals for PPO\n",
    "num_te_sorted = num_df.loc[idx_te].iloc[ord_idx]\n",
    "slack = (num_te_sorted[\"tpt_sched\"].values - num_te_sorted[\"tpt_real\"].values).astype(\"float32\")\n",
    "\n",
    "# per-route rolling late rate (window 10 within sorted order)\n",
    "late_rolling = np.zeros_like(y_seq, dtype=\"float32\")\n",
    "last_by_route = {}\n",
    "for i,(r,yv) in enumerate(zip(route, y_seq)):\n",
    "    if r not in last_by_route: last_by_route[r]=[]\n",
    "    arr = last_by_route[r]\n",
    "    arr.append(int(yv))\n",
    "    wnd = arr[-10:] if len(arr)>=10 else arr\n",
    "    late_rolling[i] = np.mean(wnd) if len(wnd)>0 else 0.0\n",
    "\n",
    "# EWMA of XGB logit as risk proxy\n",
    "xgb_sig = stack5[:,0]\n",
    "ewma = pd.Series(xgb_sig).ewm(alpha=0.2, adjust=False).mean().values.astype(\"float32\")\n",
    "\n",
    "# cyclical time features\n",
    "dow  = pd.to_datetime(tseries.iloc[ord_idx]).dt.dayofweek.values\n",
    "hour = pd.to_datetime(tseries.iloc[ord_idx]).dt.hour.values\n",
    "def cyc(a, K): \n",
    "    return np.stack([np.sin(2*np.pi*a/K), np.cos(2*np.pi*a/K)], axis=1).astype(\"float32\")\n",
    "dow2 = cyc(dow, 7); hour2 = cyc(hour, 24)\n",
    "\n",
    "# value for FN scaling\n",
    "if col_val and col_val in df.columns:\n",
    "    value = df.loc[idx_te, col_val].fillna(0).values.astype(\"float32\")[ord_idx]\n",
    "else:\n",
    "    value = np.ones_like(y_seq, dtype=\"float32\")\n",
    "v90 = np.percentile(value, 90) if value.size else 1.0\n",
    "value = (value / (v90 + 1e-6)).clip(0.5, 3.0).astype(\"float32\")\n",
    "\n",
    "# episodes: contiguous blocks of same (route, month), min length\n",
    "MIN_LEN=6\n",
    "episodes=[]; start=0\n",
    "def same_pair(i,j): return (route[i]==route[j]) and (month[i]==month[j])\n",
    "for i in range(1,len(y_seq)+1):\n",
    "    if i==len(y_seq) or not same_pair(i-1,i):\n",
    "        if i-start >= MIN_LEN:\n",
    "            episodes.append(slice(start,i))\n",
    "        start=i\n",
    "if not episodes: episodes=[slice(0,len(y_seq))]\n",
    "\n",
    "# ============== PPO Env ==============\n",
    "class SeqEnv(gym.Env):\n",
    "    metadata={\"render_modes\":[]}\n",
    "    def __init__(self, X, y, episodes, slack, late_roll, ewma, dow2, hour2, value,\n",
    "                 K=5, init_pen=0.002, final_pen=0.02, anneal_steps=80_000):\n",
    "        super().__init__()\n",
    "        self.X, self.y = X, y.astype(int)\n",
    "        self.episodes = episodes\n",
    "        self.slack = slack; self.late_roll=late_roll; self.ewma=ewma\n",
    "        self.dow2=dow2; self.hour2=hour2; self.value=value\n",
    "        self.K=K; self.init_pen=init_pen; self.final_pen=final_pen; self.anneal_steps=anneal_steps\n",
    "        # obs: 5 scores + 2(pos) + K(hist) + 2(FP/FN) + 1(slack) + 1(roll) + 1(ewma) + 2(dow) + 2(hour) = 5+2+K+2+1+1+1+2+2\n",
    "        self.obs_dim = 5 + 2 + K + 2 + 1 + 1 + 1 + 2 + 2\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.obs_dim,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self._ep=-1; self.global_steps=0\n",
    "\n",
    "    def _tfeat(self, t, T):\n",
    "        pos = t/max(T-1,1)\n",
    "        return np.array([np.sin(2*np.pi*pos), np.cos(2*np.pi*pos)], dtype=np.float32)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self._ep = (self._ep+1) % len(self.episodes)\n",
    "        sl = self.episodes[self._ep]\n",
    "        self.idx = np.arange(sl.start, sl.stop, dtype=int)\n",
    "        self.t=0; self.last=np.zeros(self.K,dtype=np.float32)\n",
    "        self.fp=0.0; self.fn=0.0; self.prev_fn=0.0\n",
    "        self.tp=0.0; self.ppo_preds=[]\n",
    "        return self._obs(), {}\n",
    "\n",
    "    def _obs(self):\n",
    "        T=len(self.idx); i=self.idx[self.t]\n",
    "        return np.concatenate([\n",
    "            self.X[i], self._tfeat(self.t,T), self.last,\n",
    "            np.array([self.fp,self.fn],dtype=np.float32),\n",
    "            np.array([self.slack[i], self.late_roll[i], self.ewma[i]], dtype=np.float32),\n",
    "            self.dow2[i], self.hour2[i]\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "    def step(self, a):\n",
    "        i=self.idx[self.t]; yv=self.y[i]\n",
    "        # TP/TN/FP/FN rewards with heavy FN penalty scaled by value & duration\n",
    "        dur = 1.0 + np.clip(float(self.slack[i] < 0) * (-self.slack[i]), 0, 10.0)  # if real>sched, duration magnitude\n",
    "        if   (a==1 and yv==1): r = 3.0; self.tp += 1.0\n",
    "        elif (a==0 and yv==0): r = 1.0\n",
    "        elif (a==1 and yv==0): r = -2.0\n",
    "        else:                  r = -10.0 * self.value[i] * (1.0 + 0.1*dur)  # **very strong**\n",
    "        # curriculum step penalty\n",
    "        pen = self.init_pen + (self.final_pen - self.init_pen) * min(1.0, self.global_steps/self.anneal_steps)\n",
    "        r -= pen\n",
    "        # shaping: reward FN-rate improvement\n",
    "        self.fn += float(yv==1 and a==0); self.fp += float(yv==0 and a==1)\n",
    "        steps=float(self.t+1); fn_rate=self.fn/max(steps,1.0)\n",
    "        if fn_rate < self.prev_fn: r += 0.2\n",
    "        self.prev_fn = fn_rate\n",
    "        # discourage over-streak of \"late\"\n",
    "        if self.t>=3 and np.allclose(self.last[-3:], 1.0): r -= 0.05\n",
    "\n",
    "        self.last = np.roll(self.last, -1); self.last[-1]=float(a)\n",
    "        self.ppo_preds.append(a)\n",
    "        self.t += 1\n",
    "        done = self.t >= len(self.idx)\n",
    "\n",
    "        # terminal bonuses: emphasize late-class Recall/F1\n",
    "        if done:\n",
    "            T = len(self.idx)\n",
    "            idxs = self.idx\n",
    "            y_ep = self.y[idxs]; a_ep = np.array(self.ppo_preds, dtype=int)\n",
    "            # per-episode confusion\n",
    "            tp = np.sum((a_ep==1) & (y_ep==1)); fn = np.sum((a_ep==0) & (y_ep==1))\n",
    "            fp = np.sum((a_ep==1) & (y_ep==0)); tn = np.sum((a_ep==0) & (y_ep==0))\n",
    "            rec = tp / max(tp+fn, 1)\n",
    "            prec= tp / max(tp+fp, 1)\n",
    "            f1 = 2*prec*rec / max(prec+rec, 1e-9)\n",
    "            # bonuses\n",
    "            r += 2.0 * rec              # strong recall bonus\n",
    "            r += 1.0 * f1               # F1 shaping\n",
    "            if rec >= 0.90: r += 1.0\n",
    "            if f1  >= 0.90: r += 0.5\n",
    "\n",
    "        obs = np.zeros(self.observation_space.shape[0], dtype=np.float32) if done else self._obs()\n",
    "        self.global_steps += 1\n",
    "        return obs, float(r), done, False, {}\n",
    "\n",
    "# ============== Make vectorized envs ==============\n",
    "def make_env():\n",
    "    return SeqEnv(stack5, y_seq, episodes, slack, late_rolling, ewma, dow2, hour2, value, K=5)\n",
    "\n",
    "n_envs=4\n",
    "venv = make_vec_env(make_env, n_envs=n_envs)\n",
    "venv = VecMonitor(venv)\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=False, clip_obs=10.0)\n",
    "\n",
    "# ============== PPO Agent (exploratory but stable) ==============\n",
    "ppo = PPO(\n",
    "    \"MlpPolicy\", venv, seed=42, verbose=0, device=rl_device,\n",
    "    batch_size=8192, n_steps=2048, learning_rate=3e-4,\n",
    "    gamma=0.995, gae_lambda=0.95, clip_range=0.2,\n",
    "    ent_coef=0.03, vf_coef=0.5, n_epochs=12,\n",
    "    target_kl=0.02, policy_kwargs=dict(net_arch=[128,128], ortho_init=True)\n",
    ")\n",
    "\n",
    "PPO_STEPS = 300_000\n",
    "t0=time.time(); venv.training=True\n",
    "ppo.learn(total_timesteps=PPO_STEPS)\n",
    "print(f\"[TIME] PPO learn: {time.time()-t0:.1f}s\")\n",
    "venv.save(\"ppo_vecnorm.pkl\")\n",
    "\n",
    "# ============== Deterministic rollout (same norm stats) ==============\n",
    "eval_env = make_vec_env(make_env, n_envs=1)\n",
    "eval_env = VecMonitor(eval_env)\n",
    "eval_env = VecNormalize.load(\"ppo_vecnorm.pkl\", eval_env)\n",
    "eval_env.training=False\n",
    "\n",
    "obs = eval_env.reset(); preds=[]\n",
    "total_steps = sum(ep.stop-ep.start for ep in episodes)\n",
    "for _ in range(total_steps):\n",
    "    a,_ = ppo.predict(obs, deterministic=True)\n",
    "    preds.append(int(a))\n",
    "    obs,_,dones,_ = eval_env.step(a)\n",
    "    if dones: obs = eval_env.reset()\n",
    "preds = np.array(preds[:len(y_seq)], dtype=int)\n",
    "\n",
    "# ============== Static ensemble comparator ==============\n",
    "# Strong static comparator: mean of expert logits -> sigmoid -> 0/1 @ 0.5\n",
    "def sigmoid(z): return 1/(1+np.exp(-z))\n",
    "ens_logit = stack_logits[ord_idx][:,:5].mean(axis=1)  # average of 5 logits aligned with ord_idx\n",
    "ens_pred  = (sigmoid(ens_logit) >= 0.5).astype(int)\n",
    "\n",
    "# ============== Metrics: overall + per-class + delta ==============\n",
    "def report(name, ytrue, yhat):\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(\"Acc/Prec/Rec/F1 (weighted):\",\n",
    "          {k:round(v,6) for k,v in metric_dict(ytrue,yhat).items()})\n",
    "    print(classification_report(ytrue, yhat, digits=4))\n",
    "    cm = confusion_matrix(ytrue, yhat)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(ytrue, yhat),\n",
    "        \"Precision\": precision_score(ytrue, yhat, average=\"weighted\", zero_division=1),\n",
    "        \"Recall\": recall_score(ytrue, yhat, average=\"weighted\", zero_division=1),\n",
    "        \"F1\": f1_score(ytrue, yhat, average=\"weighted\", zero_division=1),\n",
    "        \"Recall_late\": recall_score(ytrue, yhat, labels=[1], average=None, zero_division=1)[0] if 1 in np.unique(ytrue) else np.nan\n",
    "    }\n",
    "\n",
    "# also collect expert single-model stats (prob->0/1)\n",
    "res = {}\n",
    "res[\"XGBoost\"] = report(\"XGBoost\", y_te, (proba_xgb>=0.5).astype(int))\n",
    "for k in [\"CNN\",\"LSTM\",\"Bi-LSTM\",\"Stacked LSTM\"]:\n",
    "    res[k] = report(k, y_te, (dl_outs[k]>=0.5).astype(int))\n",
    "\n",
    "# Map y_seq metrics back to y_te order for fair table? Here we just compare on the same (sorted) subset:\n",
    "print(\"\\n[NOTE] PPO/Ensemble evaluated on time-sorted test sequence (same items).\")\n",
    "res[\"Static Ensemble\"] = report(\"Static Ensemble (avg logits)\", y_seq, ens_pred)\n",
    "res[\"PPO Sequential\"]  = report(\"PPO Sequential\", y_seq, preds)\n",
    "\n",
    "# concise final table\n",
    "def row(d): return [d[\"Accuracy\"], d[\"Precision\"], d[\"Recall\"], d[\"F1\"], d[\"Recall_late\"]]\n",
    "import pprint\n",
    "table = pd.DataFrame({\n",
    "    \"Model\": list(res.keys()),\n",
    "    \"Accuracy\": [res[m][\"Accuracy\"] for m in res],\n",
    "    \"Precision\": [res[m][\"Precision\"] for m in res],\n",
    "    \"Recall\": [res[m][\"Recall\"] for m in res],\n",
    "    \"F1\": [res[m][\"F1\"] for m in res],\n",
    "    \"Recall_late\": [res[m][\"Recall_late\"] for m in res],\n",
    "})\n",
    "print(\"\\n=== Summary (weighted) + Recall_late ===\")\n",
    "print(table.to_string(index=False))\n",
    "table.to_csv(\"metrics_ppov3_with_recall_late.csv\", index=False)\n",
    "\n",
    "print(f\"\\n[OK] Saved: metrics_ppov3_with_recall_late.csv\")\n",
    "print(f\"[TOTAL] {time.time()-t0_all:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd36b81b-9fa4-440c-bf4a-1b88649482f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] RL device: cpu\n",
      "[INFO] Loaded: (180519, 53)\n",
      "[INFO] TME: 0.19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 13:58:51.598064: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n",
      "2025-08-17 13:59:57.020993: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIME] PPO learn: 58.8s\n",
      "\n",
      "--- XGBoost ---\n",
      "Acc/Prec/Rec/F1 (weighted): {'Accuracy': 0.974435, 'Precision': 0.975569, 'Recall': 0.974435, 'F1': 0.974354}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9999    0.9435    0.9709     16308\n",
      "           1     0.9555    0.9999    0.9772     19796\n",
      "\n",
      "    accuracy                         0.9744     36104\n",
      "   macro avg     0.9777    0.9717    0.9740     36104\n",
      "weighted avg     0.9756    0.9744    0.9744     36104\n",
      "\n",
      "Confusion Matrix:\n",
      " [[15386   922]\n",
      " [    1 19795]]\n",
      "\n",
      "--- CNN ---\n",
      "Acc/Prec/Rec/F1 (weighted): {'Accuracy': 0.974463, 'Precision': 0.975599, 'Recall': 0.974463, 'F1': 0.974382}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9435    0.9709     16308\n",
      "           1     0.9555    1.0000    0.9772     19796\n",
      "\n",
      "    accuracy                         0.9745     36104\n",
      "   macro avg     0.9777    0.9717    0.9741     36104\n",
      "weighted avg     0.9756    0.9745    0.9744     36104\n",
      "\n",
      "Confusion Matrix:\n",
      " [[15386   922]\n",
      " [    0 19796]]\n",
      "\n",
      "--- LSTM ---\n",
      "Acc/Prec/Rec/F1 (weighted): {'Accuracy': 0.974463, 'Precision': 0.975599, 'Recall': 0.974463, 'F1': 0.974382}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9435    0.9709     16308\n",
      "           1     0.9555    1.0000    0.9772     19796\n",
      "\n",
      "    accuracy                         0.9745     36104\n",
      "   macro avg     0.9777    0.9717    0.9741     36104\n",
      "weighted avg     0.9756    0.9745    0.9744     36104\n",
      "\n",
      "Confusion Matrix:\n",
      " [[15386   922]\n",
      " [    0 19796]]\n",
      "\n",
      "--- Bi-LSTM ---\n",
      "Acc/Prec/Rec/F1 (weighted): {'Accuracy': 0.972773, 'Precision': 0.974017, 'Recall': 0.972773, 'F1': 0.972682}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9995    0.9402    0.9689     16308\n",
      "           1     0.9530    0.9996    0.9758     19796\n",
      "\n",
      "    accuracy                         0.9728     36104\n",
      "   macro avg     0.9763    0.9699    0.9724     36104\n",
      "weighted avg     0.9740    0.9728    0.9727     36104\n",
      "\n",
      "Confusion Matrix:\n",
      " [[15333   975]\n",
      " [    8 19788]]\n",
      "\n",
      "--- Stacked LSTM ---\n",
      "Acc/Prec/Rec/F1 (weighted): {'Accuracy': 0.974435, 'Precision': 0.975569, 'Recall': 0.974435, 'F1': 0.974354}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9999    0.9435    0.9709     16308\n",
      "           1     0.9555    0.9999    0.9772     19796\n",
      "\n",
      "    accuracy                         0.9744     36104\n",
      "   macro avg     0.9777    0.9717    0.9740     36104\n",
      "weighted avg     0.9756    0.9744    0.9744     36104\n",
      "\n",
      "Confusion Matrix:\n",
      " [[15386   922]\n",
      " [    1 19795]]\n",
      "\n",
      "[NOTE] Ensemble/PPO evaluated on the same time-sorted test items.\n",
      "\n",
      "--- Static Ensemble (avg logits) ---\n",
      "Acc/Prec/Rec/F1 (weighted): {'Accuracy': 0.974463, 'Precision': 0.975599, 'Recall': 0.974463, 'F1': 0.974382}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9435    0.9709     16308\n",
      "           1     0.9555    1.0000    0.9772     19796\n",
      "\n",
      "    accuracy                         0.9745     36104\n",
      "   macro avg     0.9777    0.9717    0.9741     36104\n",
      "weighted avg     0.9756    0.9745    0.9744     36104\n",
      "\n",
      "Confusion Matrix:\n",
      " [[15386   922]\n",
      " [    0 19796]]\n",
      "\n",
      "--- PPO Sequential (anti-imitation) ---\n",
      "Acc/Prec/Rec/F1 (weighted): {'Accuracy': 0.974463, 'Precision': 0.975599, 'Recall': 0.974463, 'F1': 0.974382}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9435    0.9709     16308\n",
      "           1     0.9555    1.0000    0.9772     19796\n",
      "\n",
      "    accuracy                         0.9745     36104\n",
      "   macro avg     0.9777    0.9717    0.9741     36104\n",
      "weighted avg     0.9756    0.9745    0.9744     36104\n",
      "\n",
      "Confusion Matrix:\n",
      " [[15386   922]\n",
      " [    0 19796]]\n",
      "\n",
      "=== Δ vs Static Ensemble (PPO - Ensemble) ===\n",
      "Δ Recall_late: +0.000000\n",
      "Δ FP_count   : +0\n",
      "\n",
      "=== Summary (weighted) + Recall_late + FP_count ===\n",
      "          Model  Accuracy  Precision   Recall       F1  Recall_late  FP_count\n",
      "        XGBoost  0.974435   0.975569 0.974435 0.974354     0.999949       922\n",
      "            CNN  0.974463   0.975599 0.974463 0.974382     1.000000       922\n",
      "           LSTM  0.974463   0.975599 0.974463 0.974382     1.000000       922\n",
      "        Bi-LSTM  0.972773   0.974017 0.972773 0.972682     0.999596       975\n",
      "   Stacked LSTM  0.974435   0.975569 0.974435 0.974354     0.999949       922\n",
      "Static Ensemble  0.974463   0.975599 0.974463 0.974382     1.000000       922\n",
      " PPO Sequential  0.974463   0.975599 0.974463 0.974382     1.000000       922\n",
      "\n",
      "[OK] Saved: metrics_ppov4_addvalue.csv\n",
      "[TOTAL] 133.2s\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# DataCo: Experts (XGB + 4 DL) → Sequential PPO that MUST add value\n",
    "# Upgrades:\n",
    "# - Drift-aware episodes: (route, month)\n",
    "# - Inputs: expert logits + slack + EWMA risk + rolling late-rate + DOW/HOUR\n",
    "# - Rewards: value/duration-scaled FN, TP bonus, terminal Recall/F1 bonus\n",
    "# - Anti-imitation: penalize matching static ensemble near uncertainty/drift\n",
    "# - VecNormalize, multi-env, more exploration, longer training\n",
    "# ==============================================================\n",
    "\n",
    "import os, gc, time, warnings, random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# mac stability\n",
    "os.environ[\"OMP_NUM_THREADS\"]=\"4\"; os.environ[\"OPENBLAS_NUM_THREADS\"]=\"4\"\n",
    "os.environ[\"MKL_NUM_THREADS\"]=\"4\"; os.environ[\"VECLIB_MAXIMUM_THREADS\"]=\"4\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"]=\"4\"\n",
    "\n",
    "import numpy as np; np.random.seed(42)\n",
    "import pandas as pd; random.seed(42)\n",
    "\n",
    "# --------- TensorFlow for DL heads ----------\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import mixed_precision\n",
    "try:\n",
    "    mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "except: pass\n",
    "try:\n",
    "    for g in tf.config.list_physical_devices('GPU'):\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "except: pass\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# --------- PyTorch / SB3 for PPO ----------\n",
    "import torch\n",
    "rl_device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"[INFO] RL device:\", rl_device)\n",
    "\n",
    "# Sklearn & metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             classification_report, confusion_matrix)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Gym/SB3\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize, VecMonitor\n",
    "\n",
    "# ================= A) Load / cache parquet =================\n",
    "# >>>> CHANGE THIS PATH <<<<\n",
    "file_path = \"/Users/dhadel/Downloads/DataCoSupplyChainDataset.csv\"\n",
    "parquet_path = os.path.splitext(file_path)[0] + \".parquet\"\n",
    "\n",
    "t0_all = time.time()\n",
    "if not os.path.exists(parquet_path):\n",
    "    print(\"[INFO] Converting CSV -> Parquet...\")\n",
    "    chunks = []\n",
    "    for ch in pd.read_csv(file_path, engine=\"python\", encoding=\"latin-1\",\n",
    "                          on_bad_lines=\"skip\", chunksize=20000):\n",
    "        chunks.append(ch)\n",
    "    pd.concat(chunks, ignore_index=True).to_parquet(parquet_path, index=False)\n",
    "    del chunks; gc.collect()\n",
    "\n",
    "df = pd.read_parquet(parquet_path)\n",
    "print(\"[INFO] Loaded:\", df.shape)\n",
    "\n",
    "# ================= B) Column mapping =================\n",
    "def first_col(d, names):\n",
    "    for n in names:\n",
    "        if n in d.columns: return n\n",
    "    return None\n",
    "\n",
    "col_real  = first_col(df, [\"Days for shipping (real)\", \"Days for shipping (real)_\", \"TPT\"])\n",
    "col_sched = first_col(df, [\"Days for shipment (scheduled)\", \"Days for shipment (scheduled)_\"])\n",
    "col_y     = first_col(df, [\"Late_delivery_risk\", \"Is_Late\"])\n",
    "col_qty   = first_col(df, [\"Order Item Quantity\", \"Unit quantity\", \"Quantity\"])\n",
    "col_org   = first_col(df, [\"Order City\", \"Order Region\", \"Order Country\"])\n",
    "col_dst   = first_col(df, [\"Customer City\", \"Customer Region\", \"Customer Country\"])\n",
    "col_car   = first_col(df, [\"Shipping Mode\", \"Carrier\", \"Shipment Mode\", \"Ship Mode\"])\n",
    "col_date  = first_col(df, [\"Order Date (DateOrders)\", \"Order Date\", \"DateOrders\", \"Date\"])\n",
    "col_val   = first_col(df, [\"Order Item Total\", \"Sales\"])\n",
    "\n",
    "# target if missing\n",
    "if col_y is None:\n",
    "    if (col_real is None) or (col_sched is None):\n",
    "        raise ValueError(\"Need Late_delivery_risk or both real/scheduled days.\")\n",
    "    df[\"__y__\"] = (pd.to_numeric(df[col_real], errors=\"coerce\") >\n",
    "                   pd.to_numeric(df[col_sched], errors=\"coerce\")).astype(int)\n",
    "    col_y = \"__y__\"\n",
    "\n",
    "keep = [c for c in [col_qty, col_real, col_sched, col_y, col_org, col_dst, col_car, col_date, col_val] if c]\n",
    "df = df[keep].copy()\n",
    "\n",
    "# numerics\n",
    "for c in [col_qty, col_real, col_sched]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df = df.dropna(subset=[col_y, col_real, col_sched, col_qty])\n",
    "\n",
    "y_all = df[col_y].astype(int).to_numpy()\n",
    "\n",
    "# ================= C) Base features =================\n",
    "num_df = pd.DataFrame(index=df.index)\n",
    "num_df[\"qty\"]         = df[col_qty]\n",
    "num_df[\"tpt_real\"]    = df[col_real]\n",
    "num_df[\"tpt_sched\"]   = df[col_sched]\n",
    "num_df[\"lead_dev\"]    = num_df[\"tpt_real\"] - num_df[\"tpt_sched\"]\n",
    "num_df[\"tpt_per_unit\"]= num_df[\"tpt_real\"] / np.clip(num_df[\"qty\"].replace(0, np.nan), 1, None)\n",
    "num_df[\"log_qty\"]     = np.log1p(np.clip(num_df[\"qty\"], 1, None))\n",
    "\n",
    "cat_df = pd.DataFrame(index=df.index)\n",
    "cat_df[\"org\"] = df[col_org].astype(str) if col_org else \"NA\"\n",
    "cat_df[\"dst\"] = df[col_dst].astype(str) if col_dst else \"NA\"\n",
    "cat_df[\"car\"] = df[col_car].astype(str) if col_car else \"NA\"\n",
    "\n",
    "# ================= D) Split then Target Mean Encoding =================\n",
    "X_num_all = num_df.to_numpy(dtype=\"float32\")\n",
    "X_cat_all = cat_df.copy()\n",
    "\n",
    "X_cat_tr, X_cat_te, X_num_tr, X_num_te, y_tr, y_te = train_test_split(\n",
    "    X_cat_all, X_num_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "def kfold_tme(X_cat: pd.DataFrame, y: np.ndarray, n_splits=5, alpha=10.0, seed=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    enc_maps = {col: [] for col in X_cat.columns}\n",
    "    X_enc = pd.DataFrame(index=X_cat.index, dtype=\"float32\")\n",
    "    global_mean = float(y.mean())\n",
    "    for col in X_cat.columns:\n",
    "        col_enc = pd.Series(index=X_cat.index, dtype=\"float32\")\n",
    "        for tr_idx, va_idx in skf.split(np.zeros(len(y)), y):\n",
    "            keys_tr = X_cat.iloc[tr_idx, X_cat.columns.get_loc(col)]\n",
    "            y_tr_f  = y[tr_idx]\n",
    "            stats = pd.DataFrame({\"k\": keys_tr.values, \"y\": y_tr_f}).groupby(\"k\")[\"y\"].agg([\"count\",\"mean\"])\n",
    "            smooth = (stats[\"count\"]*stats[\"mean\"] + alpha*global_mean) / (stats[\"count\"] + alpha)\n",
    "            keys_va = X_cat.iloc[va_idx, X_cat.columns.get_loc(col)]\n",
    "            col_enc.iloc[va_idx] = keys_va.map(smooth).fillna(global_mean).astype(\"float32\")\n",
    "        X_enc[col] = col_enc.values\n",
    "        stats_full = pd.DataFrame({\"k\": X_cat[col].values, \"y\": y}).groupby(\"k\")[\"y\"].agg([\"count\",\"mean\"])\n",
    "        smooth_full = (stats_full[\"count\"]*stats_full[\"mean\"] + alpha*global_mean) / (stats_full[\"count\"] + alpha)\n",
    "        enc_maps[col] = [smooth_full.to_dict(), float(global_mean)]\n",
    "    return X_enc.astype(\"float32\"), enc_maps\n",
    "\n",
    "def apply_tme(X_cat: pd.DataFrame, maps):\n",
    "    out = pd.DataFrame(index=X_cat.index, dtype=\"float32\")\n",
    "    for col in X_cat.columns:\n",
    "        m, g = maps[col]\n",
    "        out[col] = X_cat[col].map(m).fillna(g).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "t0 = time.time()\n",
    "Xtr_cat_tme, encoders = kfold_tme(X_cat_tr, y_tr, n_splits=5, alpha=10.0)\n",
    "Xte_cat_tme = apply_tme(X_cat_te, encoders)\n",
    "print(f\"[INFO] TME: {time.time()-t0:.2f}s\")\n",
    "\n",
    "Xtr_tab = np.hstack([X_num_tr, Xtr_cat_tme.to_numpy()])\n",
    "Xte_tab = np.hstack([X_num_te, Xte_cat_tme.to_numpy()])\n",
    "\n",
    "scaler_tab = StandardScaler()\n",
    "Xtr_tab_s = scaler_tab.fit_transform(Xtr_tab).astype(\"float32\")\n",
    "Xte_tab_s = scaler_tab.transform(Xte_tab).astype(\"float32\")\n",
    "\n",
    "Xtr_3d = Xtr_tab_s.reshape(-1, Xtr_tab_s.shape[1], 1)\n",
    "Xte_3d = Xte_tab_s.reshape(-1, Xte_tab_s.shape[1], 1)\n",
    "\n",
    "def ds(X, y, batch=1024, shuffle=True):\n",
    "    d = tf.data.Dataset.from_tensor_slices((X, y.astype(np.int32)))\n",
    "    if shuffle: d = d.shuffle(65536, seed=42, reshuffle_each_iteration=True)\n",
    "    return d.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "ds_tr, ds_te = ds(Xtr_3d, y_tr), ds(Xte_3d, y_te, shuffle=False)\n",
    "\n",
    "# ================= E) Metrics helpers =================\n",
    "def metric_dict(y_true, y_hat_or_proba):\n",
    "    arr = np.asarray(y_hat_or_proba).reshape(-1)\n",
    "    ypred = arr.astype(int) if set(np.unique(arr)) <= {0,1} else (arr >= 0.5).astype(int)\n",
    "    return dict(\n",
    "        Accuracy  = float(accuracy_score(y_true, ypred)),\n",
    "        Precision = float(precision_score(y_true, ypred, average=\"weighted\", zero_division=1)),\n",
    "        Recall    = float(recall_score(y_true, ypred, average=\"weighted\", zero_division=1)),\n",
    "        F1        = float(f1_score(y_true, ypred, average=\"weighted\", zero_division=1))\n",
    "    )\n",
    "\n",
    "# ================= F) Train experts =================\n",
    "pos_ratio = float(np.mean(y_tr))\n",
    "scale_pos_weight = float((1 - pos_ratio) / max(pos_ratio, 1e-6))\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=650, learning_rate=0.06, max_depth=6,\n",
    "    subsample=0.9, colsample_bytree=0.9,\n",
    "    tree_method=\"hist\", eval_metric=\"logloss\",\n",
    "    random_state=42, n_jobs=4, scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "xgb.fit(Xtr_tab, y_tr); proba_xgb = xgb.predict_proba(Xte_tab)[:,1]\n",
    "\n",
    "def build_dl(kind, d):\n",
    "    m = Sequential([Input(shape=(d,1))])\n",
    "    if kind==\"CNN\":\n",
    "        m.add(Conv1D(64, 3, activation=\"relu\")); m.add(Flatten())\n",
    "    elif kind==\"LSTM\":\n",
    "        m.add(LSTM(64))\n",
    "    elif kind==\"Stacked LSTM\":\n",
    "        m.add(LSTM(64, return_sequences=True)); m.add(LSTM(32))\n",
    "    elif kind==\"Bi-LSTM\":\n",
    "        m.add(Bidirectional(LSTM(64)))\n",
    "    m.add(Dense(1, activation=\"sigmoid\", dtype=\"float32\"))\n",
    "    m.compile(optimizer=Adam(1e-3), loss=\"binary_crossentropy\")\n",
    "    return m\n",
    "\n",
    "DL_EPOCHS=6\n",
    "dl_outs={}\n",
    "for kind in [\"CNN\",\"LSTM\",\"Bi-LSTM\",\"Stacked LSTM\"]:\n",
    "    mdl=build_dl(kind, Xtr_3d.shape[1])\n",
    "    mdl.fit(ds_tr, epochs=DL_EPOCHS, verbose=0)\n",
    "    dl_outs[kind]=mdl.predict(ds_te, verbose=0).reshape(-1)\n",
    "    del mdl; gc.collect()\n",
    "\n",
    "# ================= G) Build stacked logits for PPO =================\n",
    "def to_logit(p, eps=1e-6):\n",
    "    p = np.clip(p, eps, 1-eps)\n",
    "    return np.log(p) - np.log(1-p)\n",
    "\n",
    "stack_logits = np.vstack([\n",
    "    to_logit(proba_xgb),\n",
    "    to_logit(dl_outs[\"CNN\"]),\n",
    "    to_logit(dl_outs[\"LSTM\"]),\n",
    "    to_logit(dl_outs[\"Bi-LSTM\"]),\n",
    "    to_logit(dl_outs[\"Stacked LSTM\"])\n",
    "]).T.astype(\"float32\")\n",
    "\n",
    "scaler_stack = StandardScaler()\n",
    "stack5 = scaler_stack.fit_transform(stack_logits).astype(\"float32\")\n",
    "\n",
    "# ================= H) Drift-aware episodes: (route, month) =================\n",
    "idx_te = X_cat_te.index\n",
    "if col_date and col_date in df.columns:\n",
    "    tseries = pd.to_datetime(df.loc[idx_te, col_date], errors=\"coerce\")\n",
    "else:\n",
    "    tseries = pd.Series(pd.date_range(\"2000-01-01\", periods=len(idx_te), freq=\"H\"), index=idx_te)\n",
    "\n",
    "def s(col):\n",
    "    return df.loc[idx_te, col].astype(str) if col and col in df.columns else pd.Series(\"NA\", index=idx_te)\n",
    "\n",
    "route = s(col_org) + \"|\" + s(col_dst) + \"|\" + s(col_car)\n",
    "month = tseries.dt.to_period(\"M\").astype(str)\n",
    "\n",
    "# sort by time\n",
    "ord_idx = np.argsort(tseries.values)\n",
    "y_seq  = y_te[ord_idx]\n",
    "stack5 = stack5[ord_idx]\n",
    "route  = route.iloc[ord_idx].values\n",
    "month  = month.iloc[ord_idx].values\n",
    "\n",
    "# sequential signals\n",
    "num_te_sorted = num_df.loc[idx_te].iloc[ord_idx]\n",
    "slack = (num_te_sorted[\"tpt_sched\"].values - num_te_sorted[\"tpt_real\"].values).astype(\"float32\")\n",
    "\n",
    "# per-route rolling late rate (window=10)\n",
    "late_roll = np.zeros_like(y_seq, dtype=\"float32\")\n",
    "memo = {}\n",
    "for i,(r,yv) in enumerate(zip(route, y_seq)):\n",
    "    if r not in memo: memo[r]=[]\n",
    "    memo[r].append(int(yv))\n",
    "    wnd = memo[r][-10:] if len(memo[r])>=10 else memo[r]\n",
    "    late_roll[i] = np.mean(wnd) if wnd else 0.0\n",
    "\n",
    "# EWMA of XGB signal (use XGB logit col 0 of stack5 after std-scale)\n",
    "xgb_sig = stack5[:,0]\n",
    "ewma = pd.Series(xgb_sig).ewm(alpha=0.2, adjust=False).mean().values.astype(\"float32\")\n",
    "# convert EWMA to [0,1] \"risk\" via logistic\n",
    "ewma_sig = 1/(1+np.exp(-ewma))\n",
    "\n",
    "# cyclical DOW/HOUR\n",
    "dow  = pd.to_datetime(tseries.iloc[ord_idx]).dt.dayofweek.values\n",
    "hour = pd.to_datetime(tseries.iloc[ord_idx]).dt.hour.values\n",
    "def cyc(a, K): \n",
    "    return np.stack([np.sin(2*np.pi*a/K), np.cos(2*np.pi*a/K)], axis=1).astype(\"float32\")\n",
    "dow2 = cyc(dow, 7); hour2 = cyc(hour, 24)\n",
    "\n",
    "# value for FN scaling\n",
    "if col_val and col_val in df.columns:\n",
    "    value = df.loc[idx_te, col_val].fillna(0).values.astype(\"float32\")[ord_idx]\n",
    "else:\n",
    "    value = np.ones_like(y_seq, dtype=\"float32\")\n",
    "v90 = np.percentile(value, 90) if value.size else 1.0\n",
    "value = (value / (v90 + 1e-6)).clip(0.5, 3.0).astype(\"float32\")\n",
    "\n",
    "# episodes: contiguous (route, month) with min length\n",
    "MIN_LEN=6\n",
    "episodes=[]; start=0\n",
    "def same_pair(i,j): return (route[i]==route[j]) and (month[i]==month[j])\n",
    "for i in range(1,len(y_seq)+1):\n",
    "    if i==len(y_seq) or not same_pair(i-1,i):\n",
    "        if i-start >= MIN_LEN:\n",
    "            episodes.append(slice(start,i))\n",
    "        start=i\n",
    "if not episodes: episodes=[slice(0,len(y_seq))]\n",
    "\n",
    "# ================= I) Static ensemble (for anti-imitation) =================\n",
    "# average of expert logits → sigmoid probability → action\n",
    "ens_logit_sorted = stack_logits[ord_idx][:,:5].mean(axis=1)\n",
    "ens_prob_sorted  = 1/(1+np.exp(-ens_logit_sorted))\n",
    "ens_act_sorted   = (ens_prob_sorted >= 0.5).astype(int)\n",
    "\n",
    "# ================= J) PPO Env with anti-imitation =================\n",
    "class SeqEnv(gym.Env):\n",
    "    metadata={\"render_modes\":[]}\n",
    "    def __init__(self, X, y, episodes, slack, late_roll, ewma_sig, dow2, hour2, value,\n",
    "                 ens_prob, ens_act, K=5,\n",
    "                 init_pen=0.002, final_pen=0.02, anneal_steps=100_000):\n",
    "        super().__init__()\n",
    "        self.X, self.y = X, y.astype(int)\n",
    "        self.episodes = episodes\n",
    "        self.slack = slack; self.late_roll=late_roll; self.ewma_sig=ewma_sig\n",
    "        self.dow2=dow2; self.hour2=hour2; self.value=value\n",
    "        self.ens_prob=ens_prob; self.ens_act=ens_act\n",
    "        self.K=K; self.init_pen=init_pen; self.final_pen=final_pen; self.anneal_steps=anneal_steps\n",
    "        # obs: 5 scores + 2(pos) + K(hist) + 2(FP/FN) + 1(slack) + 1(roll) + 1(ewma) + 2(dow) + 2(hour)\n",
    "        self.obs_dim = 5 + 2 + K + 2 + 1 + 1 + 1 + 2 + 2\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.obs_dim,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self._ep=-1; self.global_steps=0\n",
    "\n",
    "    def _tfeat(self, t, T):\n",
    "        pos = t/max(T-1,1)\n",
    "        return np.array([np.sin(2*np.pi*pos), np.cos(2*np.pi*pos)], dtype=np.float32)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self._ep = (self._ep+1) % len(self.episodes)\n",
    "        sl = self.episodes[self._ep]\n",
    "        self.idx = np.arange(sl.start, sl.stop, dtype=int)\n",
    "        self.t=0; self.last=np.zeros(self.K,dtype=np.float32)\n",
    "        self.fp=0.0; self.fn=0.0; self.prev_fn=0.0; self.ppo_preds=[]\n",
    "        return self._obs(), {}\n",
    "\n",
    "    def _obs(self):\n",
    "        T=len(self.idx); i=self.idx[self.t]\n",
    "        return np.concatenate([\n",
    "            self.X[i], self._tfeat(self.t,T), self.last,\n",
    "            np.array([self.fp,self.fn],dtype=np.float32),\n",
    "            np.array([self.slack[i], self.late_roll[i], self.ewma_sig[i]], dtype=np.float32),\n",
    "            self.dow2[i], self.hour2[i]\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "    def step(self, a):\n",
    "        i=self.idx[self.t]; yv=self.y[i]\n",
    "        # Base rewards: heavy FN penalty scaled by value and duration\n",
    "        dur = 1.0 + np.clip(float(self.slack[i] < 0) * (-self.slack[i]), 0, 10.0)\n",
    "        if   (a==1 and yv==1): r = 3.0   # TP bonus\n",
    "        elif (a==0 and yv==0): r = 1.0   # TN\n",
    "        elif (a==1 and yv==0): r = -2.0  # FP\n",
    "        else:                  r = -12.0 * self.value[i] * (1.0 + 0.1*dur)  # **very strong FN**\n",
    "        # curriculum step penalty\n",
    "        pen = self.init_pen + (self.final_pen - self.init_pen) * min(1.0, self.global_steps/self.anneal_steps)\n",
    "        r -= pen\n",
    "\n",
    "        # Anti-imitation: discourage copying ensemble near uncertainty/drift\n",
    "        ens_p = self.ens_prob[i]; ens_a = self.ens_act[i]\n",
    "        uncertainty = 1.0 - min(1.0, 2.0*abs(ens_p - 0.5))     # 1 @ 0.5, 0 @ 0 or 1\n",
    "        drift = 0.5*self.late_roll[i] + 0.5*self.ewma_sig[i]   # ~[0,1], high → drift/risk\n",
    "        weight = 0.02 * (0.5*uncertainty + 0.5*drift)          # up to ~0.02\n",
    "        if a == ens_a: r -= weight\n",
    "        else:          r += weight\n",
    "\n",
    "        # shaping: reward improvement in online FN-rate\n",
    "        self.fn += float(yv==1 and a==0); self.fp += float(yv==0 and a==1)\n",
    "        steps=float(self.t+1); fn_rate=self.fn/max(steps,1.0)\n",
    "        if fn_rate < self.prev_fn: r += 0.2\n",
    "        self.prev_fn = fn_rate\n",
    "\n",
    "        # discourage \"late\" streaks without evidence\n",
    "        if self.t>=3 and np.allclose(self.last[-3:], 1.0): r -= 0.05\n",
    "\n",
    "        self.last = np.roll(self.last, -1); self.last[-1]=float(a)\n",
    "        self.ppo_preds.append(a)\n",
    "        self.t += 1\n",
    "        done = self.t >= len(self.idx)\n",
    "\n",
    "        # terminal bonuses: emphasize late recall & F1\n",
    "        if done:\n",
    "            idxs = self.idx\n",
    "            y_ep = self.y[idxs]; a_ep = np.array(self.ppo_preds, dtype=int)\n",
    "            tp = np.sum((a_ep==1) & (y_ep==1)); fn = np.sum((a_ep==0) & (y_ep==1))\n",
    "            fp = np.sum((a_ep==1) & (y_ep==0))\n",
    "            rec = tp / max(tp+fn, 1)\n",
    "            prec= tp / max(tp+fp, 1)\n",
    "            f1 = 2*prec*rec / max(prec+rec, 1e-9)\n",
    "            r += 2.5 * rec + 1.0 * f1\n",
    "            if rec >= 0.92: r += 1.0\n",
    "            if f1  >= 0.92: r += 0.5\n",
    "\n",
    "        obs = np.zeros(self.observation_space.shape[0], dtype=np.float32) if done else self._obs()\n",
    "        self.global_steps += 1\n",
    "        return obs, float(r), done, False, {}\n",
    "\n",
    "# ================= K) Vectorized envs + PPO =================\n",
    "def make_env():\n",
    "    return SeqEnv(stack5, y_seq, episodes, slack, late_roll, ewma_sig, dow2, hour2, value,\n",
    "                  ens_prob_sorted, ens_act_sorted, K=5)\n",
    "\n",
    "n_envs=4\n",
    "venv = make_vec_env(make_env, n_envs=n_envs)\n",
    "venv = VecMonitor(venv)\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=False, clip_obs=10.0)\n",
    "\n",
    "ppo = PPO(\n",
    "    \"MlpPolicy\", venv, seed=42, verbose=0, device=rl_device,\n",
    "    batch_size=8192, n_steps=2048, learning_rate=3e-4,\n",
    "    gamma=0.997, gae_lambda=0.95, clip_range=0.2,\n",
    "    ent_coef=0.05, vf_coef=0.5, n_epochs=12,\n",
    "    target_kl=0.02, policy_kwargs=dict(net_arch=[128,128], ortho_init=True)\n",
    ")\n",
    "\n",
    "PPO_STEPS = 500_000  # raise experience + exploration\n",
    "t0=time.time(); venv.training=True\n",
    "ppo.learn(total_timesteps=PPO_STEPS)\n",
    "print(f\"[TIME] PPO learn: {time.time()-t0:.1f}s\")\n",
    "venv.save(\"ppo_vecnorm_addvalue.pkl\")\n",
    "\n",
    "# ================= L) Deterministic rollout =================\n",
    "eval_env = make_vec_env(make_env, n_envs=1)\n",
    "eval_env = VecMonitor(eval_env)\n",
    "eval_env = VecNormalize.load(\"ppo_vecnorm_addvalue.pkl\", eval_env)\n",
    "eval_env.training=False\n",
    "\n",
    "obs = eval_env.reset(); preds=[]\n",
    "total_steps = sum(ep.stop-ep.start for ep in episodes)\n",
    "for _ in range(total_steps):\n",
    "    a,_ = ppo.predict(obs, deterministic=True)\n",
    "    preds.append(int(a))\n",
    "    obs,_,dones,_ = eval_env.step(a)\n",
    "    if dones: obs = eval_env.reset()\n",
    "preds = np.array(preds[:len(y_seq)], dtype=int)\n",
    "\n",
    "# ================= M) Baseline metrics + Static Ensemble =================\n",
    "def report(name, ytrue, yhat):\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(\"Acc/Prec/Rec/F1 (weighted):\",\n",
    "          {k:round(v,6) for k,v in metric_dict(ytrue,yhat).items()})\n",
    "    print(classification_report(ytrue, yhat, digits=4))\n",
    "    cm = confusion_matrix(ytrue, yhat)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    # class 1 recall\n",
    "    if 1 in np.unique(ytrue):\n",
    "        recall_late = recall_score(ytrue, yhat, labels=[1], average=None, zero_division=1)[0]\n",
    "    else:\n",
    "        recall_late = np.nan\n",
    "    # FP count\n",
    "    fp = int(((yhat==1) & (ytrue==0)).sum())\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(ytrue, yhat),\n",
    "        \"Precision\": precision_score(ytrue, yhat, average=\"weighted\", zero_division=1),\n",
    "        \"Recall\": recall_score(ytrue, yhat, average=\"weighted\", zero_division=1),\n",
    "        \"F1\": f1_score(ytrue, yhat, average=\"weighted\", zero_division=1),\n",
    "        \"Recall_late\": recall_late,\n",
    "        \"FP_count\": fp\n",
    "    }\n",
    "\n",
    "# Expert single-model summaries (prob→0/1 at 0.5)\n",
    "res = {}\n",
    "res[\"XGBoost\"] = report(\"XGBoost\", y_te, (proba_xgb>=0.5).astype(int))\n",
    "for k in [\"CNN\",\"LSTM\",\"Bi-LSTM\",\"Stacked LSTM\"]:\n",
    "    res[k] = report(k, y_te, (dl_outs[k]>=0.5).astype(int))\n",
    "\n",
    "print(\"\\n[NOTE] Ensemble/PPO evaluated on the same time-sorted test items.\")\n",
    "ens_pred = (ens_prob_sorted >= 0.5).astype(int)\n",
    "res[\"Static Ensemble\"] = report(\"Static Ensemble (avg logits)\", y_seq, ens_pred)\n",
    "res[\"PPO Sequential\"]  = report(\"PPO Sequential (anti-imitation)\", y_seq, preds)\n",
    "\n",
    "# Deltas vs Static Ensemble (the thing PPO must beat)\n",
    "delta_recall_late = res[\"PPO Sequential\"][\"Recall_late\"] - res[\"Static Ensemble\"][\"Recall_late\"]\n",
    "delta_fp          = res[\"PPO Sequential\"][\"FP_count\"]   - res[\"Static Ensemble\"][\"FP_count\"]\n",
    "\n",
    "print(\"\\n=== Δ vs Static Ensemble (PPO - Ensemble) ===\")\n",
    "print(f\"Δ Recall_late: {delta_recall_late:+.6f}\")\n",
    "print(f\"Δ FP_count   : {delta_fp:+d}\")\n",
    "\n",
    "# compact table\n",
    "table = pd.DataFrame({\n",
    "    \"Model\": list(res.keys()),\n",
    "    \"Accuracy\": [res[m][\"Accuracy\"] for m in res],\n",
    "    \"Precision\": [res[m][\"Precision\"] for m in res],\n",
    "    \"Recall\": [res[m][\"Recall\"] for m in res],\n",
    "    \"F1\": [res[m][\"F1\"] for m in res],\n",
    "    \"Recall_late\": [res[m][\"Recall_late\"] for m in res],\n",
    "    \"FP_count\": [res[m][\"FP_count\"] for m in res],\n",
    "})\n",
    "print(\"\\n=== Summary (weighted) + Recall_late + FP_count ===\")\n",
    "print(table.to_string(index=False))\n",
    "table.to_csv(\"metrics_ppov4_addvalue.csv\", index=False)\n",
    "\n",
    "print(f\"\\n[OK] Saved: metrics_ppov4_addvalue.csv\")\n",
    "print(f\"[TOTAL] {time.time()-t0_all:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53834f7-e742-42fa-8e34-5347595eadc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda3)",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
