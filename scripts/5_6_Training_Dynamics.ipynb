{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4abea060-7c9a-47bd-a765-8d7cce4bf93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration Timesteps Ep. Reward      Loss Policy Loss Value Loss  KL Div.\n",
      "0         2      4096      -0.77  -0.66532    -0.12411       6.50  0.02943\n",
      "1         6     12288       1.72  -0.04092    -0.00471       1.54  0.00117\n",
      "2         8     16384       1.76  -0.01346    -0.00242       1.45  0.00060\n",
      "3        10     20480       1.69  -0.00701    -0.00078       1.37  0.00012\n",
      "4        12     24576       1.78  -0.00771    -0.00091       1.26  0.00049\n",
      "5        14     28672       1.87  -0.00686    -0.00124       1.37  0.00059\n",
      "6        16     32768       1.79  -0.00652    -0.00098       1.35  0.00044\n",
      "7        20     40960       1.67  -0.00403    -0.00073       1.27  0.00042\n",
      "8        25     51200       1.72  -0.00624    -0.00062       1.31  0.00104\n",
      "Saved: ppo_training_metrics_selected.csv\n",
      "Saved: ppo_training_metrics_selected.tex\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Summarize PPO training metrics at selected iterations\n",
    "# =========================\n",
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "def summarize_ppo_training(\n",
    "    logdir,\n",
    "    selected_iterations=(2,6,8,10,12,14,16,20,25),\n",
    "    out_prefix=\"ppo_training_metrics_selected\"\n",
    "):\n",
    "    progress_csv = os.path.join(logdir, \"progress.csv\")\n",
    "    if not os.path.exists(progress_csv):\n",
    "        raise FileNotFoundError(f\"progress.csv not found at: {progress_csv}\")\n",
    "\n",
    "    df = pd.read_csv(progress_csv)\n",
    "\n",
    "    # Column handles (present across SB3 versions; fallbacks if missing)\n",
    "    it_col   = \"time/iterations\"      if \"time/iterations\"      in df.columns else None\n",
    "    ts_col   = \"time/total_timesteps\" if \"time/total_timesteps\" in df.columns else None\n",
    "    rew_col  = \"rollout/ep_rew_mean\"  if \"rollout/ep_rew_mean\"  in df.columns else None\n",
    "    pol_col  = \"train/policy_gradient_loss\" if \"train/policy_gradient_loss\" in df.columns else None\n",
    "    val_col  = \"train/value_loss\"            if \"train/value_loss\"            in df.columns else None\n",
    "    ent_col  = \"train/entropy_loss\"          if \"train/entropy_loss\"          in df.columns else None\n",
    "    kl_col   = \"train/approx_kl\"             if \"train/approx_kl\"             in df.columns else None\n",
    "\n",
    "    # Fallbacks (keep table usable even if some keys are absent)\n",
    "    if it_col is None:\n",
    "        df[\"__iter__\"] = np.arange(1, len(df)+1)\n",
    "        it_col = \"__iter__\"\n",
    "    if ts_col is None:\n",
    "        df[\"__ts__\"] = np.arange(1, len(df)+1)\n",
    "        ts_col = \"__ts__\"\n",
    "    if rew_col is None:\n",
    "        df[\"__rew__\"] = np.nan\n",
    "        rew_col = \"__rew__\"\n",
    "\n",
    "    def nearest_row_for_iter(target_it):\n",
    "        idx = (df[it_col] - target_it).abs().idxmin()\n",
    "        row = df.loc[idx]\n",
    "        return {\n",
    "            \"Iteration\":   int(round(row[it_col])) if pd.notnull(row[it_col]) else int(target_it),\n",
    "            \"Timesteps\":   int(round(row[ts_col])) if pd.notnull(row[ts_col]) else int(idx+1),\n",
    "            \"Ep. Reward\":  float(row[rew_col])     if pd.notnull(row[rew_col]) else np.nan,\n",
    "            # Report entropy_loss under a single \"Loss\" column to match your example\n",
    "            \"Loss\":        float(row[ent_col])     if ent_col and pd.notnull(row[ent_col]) else np.nan,\n",
    "            \"Policy Loss\": float(row[pol_col])     if pol_col and pd.notnull(row[pol_col]) else np.nan,\n",
    "            \"Value Loss\":  float(row[val_col])     if val_col and pd.notnull(row[val_col]) else np.nan,\n",
    "            \"KL Div.\":     float(row[kl_col])      if kl_col  and pd.notnull(row[kl_col])  else np.nan,\n",
    "        }\n",
    "\n",
    "    rows = [nearest_row_for_iter(it) for it in selected_iterations]\n",
    "    out = pd.DataFrame(rows)\n",
    "\n",
    "    # Format like your sample\n",
    "    def fmt(x, d=5):\n",
    "        if pd.isna(x): return \"\"\n",
    "        if isinstance(x, (int, np.integer)): return f\"{x:d}\"\n",
    "        return f\"{x:.{d}f}\"\n",
    "    out_fmt = out.copy()\n",
    "    out_fmt[\"Iteration\"]   = out_fmt[\"Iteration\"].apply(lambda v: fmt(v, 0))\n",
    "    out_fmt[\"Timesteps\"]   = out_fmt[\"Timesteps\"].apply(lambda v: fmt(v, 0))\n",
    "    out_fmt[\"Ep. Reward\"]  = out_fmt[\"Ep. Reward\"].apply(lambda v: fmt(v, 2))\n",
    "    out_fmt[\"Loss\"]        = out_fmt[\"Loss\"].apply(lambda v: fmt(v, 5))\n",
    "    out_fmt[\"Policy Loss\"] = out_fmt[\"Policy Loss\"].apply(lambda v: fmt(v, 5))\n",
    "    out_fmt[\"Value Loss\"]  = out_fmt[\"Value Loss\"].apply(lambda v: fmt(v, 2))\n",
    "    out_fmt[\"KL Div.\"]     = out_fmt[\"KL Div.\"].apply(lambda v: fmt(v, 5))\n",
    "\n",
    "    # Save\n",
    "    csv_path = f\"{out_prefix}.csv\"\n",
    "    tex_path = f\"{out_prefix}.tex\"\n",
    "    out_fmt.to_csv(csv_path, index=False)\n",
    "\n",
    "    # LaTeX (simple tabular with caption/label)\n",
    "    latex = out_fmt.to_latex(\n",
    "        index=False,\n",
    "        caption=\"Summary of the proposed PPO Agentâ€™s Training Metrics Across Selected Iterations.\",\n",
    "        label=\"tab:ppo_training_metrics\",\n",
    "        escape=False\n",
    "    )\n",
    "    with open(tex_path, \"w\") as f:\n",
    "        f.write(latex)\n",
    "\n",
    "    return out_fmt, csv_path, tex_path\n",
    "\n",
    "# ==== Usage ====\n",
    "# Use your actual log directory:\n",
    "LOGDIR = \"./logs/ppo_seq\"   # or \"./logs/ppo_static\"\n",
    "table, csv_path, tex_path = summarize_ppo_training(LOGDIR)\n",
    "\n",
    "print(table)\n",
    "print(\"Saved:\", csv_path)\n",
    "print(\"Saved:\", tex_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8491f375-2160-440b-8a82-05d0acfcd1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time-steps  Ep. Len  Ep. Reward  Policy Grad. Loss  Value Loss  \\\n",
      "0        6144     1.39     -0.1759          -0.141333    5.414640   \n",
      "1       14336     1.56      1.6464          -0.004921    1.482717   \n",
      "2       18432     1.44      1.7356          -0.002416    1.528543   \n",
      "3       22528     1.68      1.8552          -0.001066    1.382684   \n",
      "4       26624     1.34      1.4866          -0.000679    1.237726   \n",
      "5       30720     1.40      1.6060          -0.000026    1.355595   \n",
      "6       34816     1.54      1.7146          -0.000293    1.224432   \n",
      "7       43008     1.52      1.8148          -0.000605    1.222519   \n",
      "8       53248     1.70      1.9430          -0.000484    1.315318   \n",
      "\n",
      "   Entropy Loss  KL Divergence  Total Loss  \n",
      "0     -0.565204       0.038563    4.708103  \n",
      "1     -0.021416       0.002808    1.456380  \n",
      "2     -0.009052       0.001057    1.517075  \n",
      "3     -0.005384       0.000518    1.376234  \n",
      "4     -0.006442       0.000410    1.230606  \n",
      "5     -0.007462       0.000164    1.348107  \n",
      "6     -0.004840       0.000116    1.219300  \n",
      "7     -0.006205       0.000273    1.215709  \n",
      "8     -0.003801       0.000245    1.311033  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to your PPO log folder (where progress.csv is saved)\n",
    "logdir = \"./logs/ppo_seq\"  # change to your PPO run directory\n",
    "progress_csv = os.path.join(logdir, \"progress.csv\")\n",
    "\n",
    "# Read progress.csv\n",
    "df = pd.read_csv(progress_csv)\n",
    "\n",
    "# Columns to extract (adjust names if SB3 version changes)\n",
    "cols_map = {\n",
    "    \"Time-steps\": \"time/total_timesteps\",\n",
    "    \"Ep. Len\": \"rollout/ep_len_mean\",\n",
    "    \"Ep. Reward\": \"rollout/ep_rew_mean\",\n",
    "    \"Policy Grad. Loss\": \"train/policy_gradient_loss\",\n",
    "    \"Value Loss\": \"train/value_loss\",\n",
    "    \"Entropy Loss\": \"train/entropy_loss\",\n",
    "    \"KL Divergence\": \"train/approx_kl\"\n",
    "}\n",
    "\n",
    "# Create output DataFrame\n",
    "out_df = pd.DataFrame()\n",
    "for pretty, col in cols_map.items():\n",
    "    if col in df.columns:\n",
    "        out_df[pretty] = df[col]\n",
    "    else:\n",
    "        out_df[pretty] = np.nan  # fill if missing\n",
    "\n",
    "# Compute Total Loss (sum of policy, value, and entropy losses)\n",
    "out_df[\"Total Loss\"] = (\n",
    "    out_df[\"Policy Grad. Loss\"].fillna(0) +\n",
    "    out_df[\"Value Loss\"].fillna(0) +\n",
    "    out_df[\"Entropy Loss\"].fillna(0)\n",
    ")\n",
    "\n",
    "# Optionally pick specific iterations (example: every 2nd row up to 25 rows)\n",
    "selected_rows = [2,6,8,10,12,14,16,20,25]\n",
    "summary_df = out_df.iloc[selected_rows].reset_index(drop=True)\n",
    "\n",
    "# Save CSV and LaTeX table\n",
    "summary_df.to_csv(\"ppo_training_summary.csv\", index=False)\n",
    "with open(\"ppo_training_summary.tex\", \"w\") as f:\n",
    "    f.write(summary_df.to_latex(index=False, caption=\"PPO Training Metrics Summary\", label=\"tab:ppo_metrics\", escape=False))\n",
    "\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8a551640-c8bf-4f00-bb20-caac18283506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] ppo_selected_epreward.png / .pdf\n",
      "[Saved] ppo_selected_kl.png / .pdf\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Journal-quality plots from ppo_training_metrics_selected.csv\n",
    "# - Episode Reward vs. Training Steps\n",
    "# - KL Divergence vs. Training Steps\n",
    "# Exports: PNG (300 DPI) + PDF (vector)\n",
    "# ============================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "CSV_PATH   = \"ppo_training_metrics_selected.csv\"  # <- set if different\n",
    "OUT_PREFIX = \"ppo_selected\"                       # output filename prefix\n",
    "SMOOTH_FRAC = 0.3   # 0 = raw only; 0.2â€“0.4 works well for sparse points\n",
    "SHOW_POINTS = True  # draw small markers at actual points\n",
    "\n",
    "# ---- Matplotlib defaults (journal style) ----\n",
    "mpl.rcParams.update({\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"font.size\": 9,\n",
    "    \"axes.labelsize\": 9,\n",
    "    \"axes.titlesize\": 9,\n",
    "    \"xtick.labelsize\": 8,\n",
    "    \"ytick.labelsize\": 8,\n",
    "    \"legend.fontsize\": 8,\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"ps.fonttype\": 42,\n",
    "    \"font.family\": \"serif\",\n",
    "})\n",
    "\n",
    "def _rolling_centered(series: pd.Series, frac: float):\n",
    "    \"\"\"Centered rolling mean with a safe window for few points.\"\"\"\n",
    "    if not frac or frac <= 0 or len(series) < 5:\n",
    "        return series\n",
    "    w = max(3, int(round(len(series) * frac)))\n",
    "    return series.rolling(w, center=True, min_periods=max(1, w//2)).mean()\n",
    "\n",
    "def _pick(df: pd.DataFrame, candidates):\n",
    "    \"\"\"Pick the first existing column from candidates.\"\"\"\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _format_thousands(ax):\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, pos: f\"{int(x):,}\" if x == int(x) else f\"{x:,.0f}\"))\n",
    "\n",
    "def _style_axes(ax, xlabel, ylabel):\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    _format_thousands(ax)\n",
    "    ax.grid(True, lw=0.4, alpha=0.45)\n",
    "    for spine in [\"top\", \"right\"]:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "\n",
    "def _plot_series(x, y, ylabel, out_suffix):\n",
    "    # Clean NaNs and sort\n",
    "    m = ~(x.isna() | y.isna())\n",
    "    x, y = x[m], y[m]\n",
    "    if len(x) == 0:\n",
    "        print(f\"[Warn] No data for {ylabel}; skipping.\")\n",
    "        return\n",
    "    order = np.argsort(x.values)\n",
    "    x = x.iloc[order].astype(float)\n",
    "    y = y.iloc[order].astype(float)\n",
    "\n",
    "    y_s = _rolling_centered(y, SMOOTH_FRAC)\n",
    "\n",
    "    fig = plt.figure(figsize=(3.5, 2.4))  # single-column figure\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # Raw (faint) + smoothed (bold)\n",
    "    if SHOW_POINTS:\n",
    "        ax.plot(x, y, lw=0.9, alpha=0.35, marker='o', markersize=3, label=\"raw\")\n",
    "    else:\n",
    "        ax.plot(x, y, lw=0.9, alpha=0.35, label=\"raw\")\n",
    "\n",
    "    ax.plot(x, y_s if len(y_s) else y, lw=1.9, label=\"smoothed\")\n",
    "\n",
    "    # Labels, grid, legend\n",
    "    _style_axes(ax, \"Training steps\", ylabel)\n",
    "    ax.legend(loc=\"best\", frameon=False)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"{OUT_PREFIX}_{out_suffix}.png\", bbox_inches=\"tight\")\n",
    "    fig.savefig(f\"{OUT_PREFIX}_{out_suffix}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"[Saved] {OUT_PREFIX}_{out_suffix}.png / .pdf\")\n",
    "\n",
    "# ======== Load and resolve columns ========\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "steps_col = _pick(df, [\"Timesteps\", \"Time-steps\", \"time/total_timesteps\", \"Steps\"])\n",
    "rew_col   = _pick(df, [\"Ep. Reward\", \"Episode Reward\", \"rollout/ep_rew_mean\"])\n",
    "kl_col    = _pick(df, [\"KL Divergence\", \"KL Div.\", \"train/approx_kl\"])\n",
    "\n",
    "# Fallback: if no steps column, use row index\n",
    "if steps_col is None:\n",
    "    df[\"__steps__\"] = np.arange(1, len(df) + 1)\n",
    "    steps_col = \"__steps__\"\n",
    "\n",
    "# Cast numeric\n",
    "for c in [steps_col, rew_col, kl_col]:\n",
    "    if c is not None:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# ======== Plot figures ========\n",
    "if rew_col is not None:\n",
    "    _plot_series(df[steps_col], df[rew_col], ylabel=\"Episode reward (mean)\", out_suffix=\"epreward\")\n",
    "else:\n",
    "    print(\"[Warn] No episode reward column found.\")\n",
    "\n",
    "if kl_col is not None:\n",
    "    _plot_series(df[steps_col], df[kl_col], ylabel=\"Approx. KL divergence\", out_suffix=\"kl\")\n",
    "else:\n",
    "    print(\"[Warn] No KL divergence column found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1bc4fd11-d919-4259-8293-dd438256c73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] ppo_selected_epreward.png / .pdf\n",
      "[Saved] ppo_selected_kl.png / .pdf\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# PPO plots from ppo_training_metrics_selected.csv\n",
    "# - Episode Reward vs. Training Steps\n",
    "# - KL Divergence vs. Training Steps\n",
    "# No smoothing, smaller labels, PNG 300 DPI + PDF\n",
    "# ============================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "CSV_PATH   = \"ppo_training_metrics_selected.csv\"  # change if needed\n",
    "OUT_PREFIX = \"ppo_selected\"                       # output file prefix\n",
    "\n",
    "# --- Compact journal-ish style (smaller labels) ---\n",
    "mpl.rcParams.update({\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"font.size\": 8,\n",
    "    \"axes.labelsize\": 8,\n",
    "    \"axes.titlesize\": 8,\n",
    "    \"xtick.labelsize\": 7,\n",
    "    \"ytick.labelsize\": 7,\n",
    "    \"legend.fontsize\": 7,\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"ps.fonttype\": 42,\n",
    "    \"font.family\": \"serif\",\n",
    "})\n",
    "\n",
    "def pick_col(df: pd.DataFrame, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def format_thousands(ax):\n",
    "    ax.xaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda x, pos: f\"{int(x):,}\" if x == int(x) else f\"{x:,.0f}\")\n",
    "    )\n",
    "\n",
    "def style_axes(ax, xlabel, ylabel):\n",
    "    ax.set_xlabel(xlabel, labelpad=2)\n",
    "    ax.set_ylabel(ylabel, labelpad=2)\n",
    "    ax.grid(True, lw=0.35, alpha=0.45)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    format_thousands(ax)\n",
    "\n",
    "def plot_line(x, y, ylabel, suffix):\n",
    "    m = ~(x.isna() | y.isna())\n",
    "    x, y = x[m], y[m]\n",
    "    if len(x) == 0:\n",
    "        print(f\"[Warn] No data for {suffix}; skipping.\")\n",
    "        return\n",
    "    order = np.argsort(x.values)\n",
    "    x = x.iloc[order].astype(float)\n",
    "    y = y.iloc[order].astype(float)\n",
    "\n",
    "    fig = plt.figure(figsize=(3.3, 2.2))  # a bit smaller than before\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(x, y, lw=1.4)                 # single clean line, no markers\n",
    "    style_axes(ax, \"Training steps\", ylabel)\n",
    "    fig.tight_layout(pad=0.6)\n",
    "    fig.savefig(f\"{OUT_PREFIX}_{suffix}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    fig.savefig(f\"{OUT_PREFIX}_{suffix}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"[Saved] {OUT_PREFIX}_{suffix}.png / .pdf\")\n",
    "\n",
    "# ---- Load CSV ----\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Resolve columns (robust to name variants)\n",
    "steps_col = pick_col(df, [\"Timesteps\", \"Time-steps\", \"time/total_timesteps\", \"Steps\"])\n",
    "rew_col   = pick_col(df, [\"Ep. Reward\", \"Episode Reward\", \"rollout/ep_rew_mean\"])\n",
    "kl_col    = pick_col(df, [\"KL Divergence\", \"KL Div.\", \"train/approx_kl\"])\n",
    "\n",
    "# Fallback for steps\n",
    "if steps_col is None:\n",
    "    df[\"__steps__\"] = np.arange(1, len(df) + 1)\n",
    "    steps_col = \"__steps__\"\n",
    "\n",
    "# Cast numeric\n",
    "for c in [steps_col, rew_col, kl_col]:\n",
    "    if c is not None:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# --- Plot Episode Reward ---\n",
    "if rew_col is not None:\n",
    "    plot_line(df[steps_col], df[rew_col], ylabel=\"Episode reward (mean)\", suffix=\"epreward\")\n",
    "else:\n",
    "    print(\"[Warn] Episode reward column not found; skipping.\")\n",
    "\n",
    "# --- Plot KL Divergence ---\n",
    "if kl_col is not None:\n",
    "    plot_line(df[steps_col], df[kl_col], ylabel=\"Approx. KL divergence\", suffix=\"kl\")\n",
    "else:\n",
    "    print(\"[Warn] KL divergence column not found; skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab08de-9ddb-425e-8614-80d4f80afcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda3)",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
