{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e6ea4-dcbe-463d-aa52-976c719bd53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented features detected: ['adaptive_sla', 'congestion_flag', 'disruption_flag', 'time_drift', 'OrderMonth', 'OrderWeek', 'OrderWeekday', 'is_weekend', 'Weight_per_Unit', 'TPT_per_Unit', 'LeadTime_Deviation', 'Weight_vsCarrierMean', 'Unit_vsCarrierMean', 'Carrier_Origin', 'Plant_Destination', 'log_Weight', 'log_UnitQty', 'congestion_trend', 'sla_risk_trend', 'disruption_trend', 'time_drift_abs', 'carrier_origin_risk', 'route_cum_late_rate', 'route_bb_mean', 'carrier_cum_late_rate', 'carrier_bb_mean', 'seq_pos_norm', 'interarrival_idx_gap', 'route_roll10_TPT_mean', 'route_roll10_TPT_std', 'route_roll10_TPT_q90', 'route_roll10_LeadTime_Deviation_mean', 'route_roll10_LeadTime_Deviation_std', 'route_roll10_LeadTime_Deviation_q90', 'route_roll10_Weight_mean', 'route_roll10_Weight_std', 'route_roll10_Weight_q90', 'route_orders_last7d', 'route_orders_last14d', 'route_orders_last30d']\n",
      "Features of interest (present): ['Origin Port', 'Carrier', 'Plant Code', 'Destination Port', 'Plant_Destination', 'Unit quantity', 'Weight', 'TPT', 'TPT_per_Unit', 'LeadTime_Deviation', 'Weight_per_Unit', 'log_UnitQty', 'carrier_origin_risk', 'route_cum_late_rate', 'route_bb_mean', 'carrier_bb_mean', 'route_orders_last7d', 'route_roll10_Weight_q90', 'congestion_trend', 'Weight_vsCarrierMean', 'seq_pos_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cm/x59s6wt92191zsp8lhjyn09r0000gn/T/ipykernel_61253/3808962942.py:243: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
      "  shap.summary_plot(shap_values, X_train_trans[:ns], feature_names=feature_names_final, show=False, max_display=N_TOP_PLOT)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved outputs in: /Users/dhadel/empirical_eval_outputs\n",
      "Tables:\n",
      " - Empirical_Stats_Augmented.(xlsx/csv)\n",
      " - Empirical_Stats_FeaturesOfInterest.(xlsx/csv)\n",
      " - SHAP_Importance_Aggregated.(xlsx/csv)\n",
      " - SHAP_Importance_FeaturesOfInterest.(xlsx/csv)\n",
      " - Empirical_Eval_FeaturesOfInterest.(xlsx/csv)\n",
      "Figures:\n",
      " - SHAP_SummaryBar_Top20.png\n",
      " - SHAP_Beeswarm_Top20.png\n",
      " - dependence_plots/*.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.stats import chi2_contingency, pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import __version__ as sklver\n",
    "from xgboost import XGBClassifier\n",
    "file_path = os.path.join(\"Data\", \"Hybrid_Augmented_TSAFE_Features.xlsx\")\n",
    "OUTDIR = Path(\"./empirical_eval_outputs\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "DPI = 600\n",
    "N_TOP_PLOT = 20\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "df = pd.read_excel(file_path)\n",
    "if 'Order Date' in df.columns:\n",
    "    df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
    "    df = df.sort_values(['Order Date']).reset_index(drop=True)\n",
    "\n",
    "# 1.1) Ensure Plant_Destination (your new rule)\n",
    "if 'Plant_Destination' not in df.columns:\n",
    "    if {'Plant Code', 'Destination Port'}.issubset(df.columns):\n",
    "        df['Plant_Destination'] = (\n",
    "            df['Plant Code'].astype(str) + ' | ' + df['Destination Port'].astype(str)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Cannot create 'Plant_Destination' because 'Plant Code' or 'Destination Port' is missing.\")\n",
    "\n",
    "\n",
    "original_features = [\n",
    "    'Order ID','Order Date','Origin Port','Carrier','TPT','Service Level',\n",
    "    'Ship ahead day count','Ship Late Day count','Customer','Product ID',\n",
    "    'Plant Code','Destination Port','Unit quantity','Weight'\n",
    "]\n",
    "all_features = df.columns.tolist()\n",
    "augmented_features = [c for c in all_features if c not in original_features]\n",
    "\n",
    "\n",
    "features_of_interest_cat = [\n",
    "    'Origin Port', 'Carrier', 'Plant Code', 'Destination Port', 'Plant_Destination'\n",
    "]\n",
    "features_of_interest_num = [\n",
    "    'Unit quantity', 'Weight', 'TPT',\n",
    "    'TPT_per_Unit', 'LeadTime_Deviation', 'Weight_per_Unit',\n",
    "    'log_UnitQty', 'carrier_origin_risk', 'route_cum_late_rate',\n",
    "    'route_bb_mean', 'carrier_bb_mean', 'route_orders_last7d',\n",
    "    'route_roll10_Weight_q90', 'congestion_trend', 'Weight_vsCarrierMean',\n",
    "    'seq_pos_norm'\n",
    "]\n",
    "\n",
    "features_of_interest_cat = [c for c in features_of_interest_cat if c in df.columns]\n",
    "features_of_interest_num = [c for c in features_of_interest_num if c in df.columns]\n",
    "features_of_interest = features_of_interest_cat + features_of_interest_num\n",
    "\n",
    "print(\"Augmented features detected:\", augmented_features)\n",
    "print(\"Features of interest (present):\", features_of_interest)\n",
    "\n",
    "\n",
    "if 'Ship Late Day count' not in df.columns:\n",
    "    raise ValueError(\"Missing 'Ship Late Day count' for target.\")\n",
    "y = (df['Ship Late Day count'] > 0).astype(int)\n",
    "\n",
    "\n",
    "X_aug = df[augmented_features].copy()\n",
    "X_aug = X_aug.replace([np.inf, -np.inf], np.nan)\n",
    "X_aug_clean = X_aug.apply(lambda col: col.fillna(col.median()) if col.dtype != 'object' else col)\n",
    "\n",
    "\n",
    "def cramers_v_from_chi2(chi2, n, k_rows, k_cols):\n",
    "    # Bias-corrected Cramér’s V\n",
    "    phi2 = chi2 / n\n",
    "    r, k = k_rows, k_cols\n",
    "    phi2corr = max(0, phi2 - (k-1)*(r-1)/(n-1))\n",
    "    rcorr = r - (r-1)**2/(n-1)\n",
    "    kcorr = k - (k-1)**2/(n-1)\n",
    "    denom = min(kcorr-1, rcorr-1)\n",
    "    return np.sqrt(phi2corr / denom) if denom > 0 else 0.0\n",
    "\n",
    "stat_rows = []\n",
    "for feat in augmented_features:\n",
    "    s = X_aug_clean[feat]\n",
    "    try:\n",
    "        if s.dtype == 'object' or s.nunique() <= 10:\n",
    "            tbl = pd.crosstab(s, y)\n",
    "            chi2, p, dof, exp = chi2_contingency(tbl)\n",
    "            V = cramers_v_from_chi2(chi2, n=len(df), k_rows=tbl.shape[0], k_cols=tbl.shape[1])\n",
    "            stat_rows.append([feat, 'Chi-Square', chi2, p, V, 'Cramér_V'])\n",
    "        else:\n",
    "            valid = s.dropna().index\n",
    "            r, p = pearsonr(s.loc[valid], y.loc[valid])\n",
    "            stat_rows.append([feat, 'Point-Biserial (Pearson)', r, p, abs(r), '|r_pb|'])\n",
    "    except Exception as e:\n",
    "        stat_rows.append([feat, 'ERROR', np.nan, np.nan, np.nan, str(e)])\n",
    "\n",
    "stat_df = pd.DataFrame(stat_rows, columns=['Feature', 'Test', 'Statistic', 'p_value', 'EffectSize', 'EffectType'])\n",
    "mask_ok = stat_df['p_value'].notna()\n",
    "stat_df.loc[mask_ok, 'q_value'] = multipletests(stat_df.loc[mask_ok, 'p_value'].values, method='fdr_bh')[1]\n",
    "stat_df = stat_df.sort_values(['q_value', 'p_value'])\n",
    "\n",
    "\n",
    "stat_df.to_excel(OUTDIR / \"Empirical_Stats_Augmented.xlsx\", index=False)\n",
    "stat_df.to_csv(OUTDIR / \"Empirical_Stats_Augmented.csv\", index=False)\n",
    "stat_focus = stat_df[stat_df['Feature'].isin(features_of_interest)].copy()\n",
    "stat_focus.to_excel(OUTDIR / \"Empirical_Stats_FeaturesOfInterest.xlsx\", index=False)\n",
    "stat_focus.to_csv(OUTDIR / \"Empirical_Stats_FeaturesOfInterest.csv\", index=False)\n",
    "\n",
    "\n",
    "cat_features = [c for c in features_of_interest_cat if c in df.columns]\n",
    "num_features = [c for c in features_of_interest_num if c in df.columns]\n",
    "X_base = df[cat_features + num_features].copy()\n",
    "\n",
    "if 'Order Date' in df.columns:\n",
    "    cutoff = df['Order Date'].quantile(0.8)\n",
    "    train_idx = df['Order Date'] <= cutoff\n",
    "    test_idx  = df['Order Date'] >  cutoff\n",
    "else:\n",
    "    train_idx, test_idx = train_test_split(df.index, test_size=0.2, stratify=y, random_state=SEED)\n",
    "\n",
    "X_train = X_base.loc[train_idx]\n",
    "X_test  = X_base.loc[test_idx]\n",
    "y_train = y.loc[train_idx]\n",
    "y_test  = y.loc[test_idx]\n",
    "\n",
    "\n",
    "major, minor = map(int, sklver.split('.')[:2])\n",
    "if (major, minor) >= (1, 2):\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "else:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', ohe, cat_features),\n",
    "        ('num', 'passthrough', num_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "    subsample=0.9, colsample_bytree=0.9,\n",
    "    eval_metric='logloss', random_state=SEED\n",
    ")\n",
    "\n",
    "pipe = Pipeline([('pre', pre), ('clf', xgb)])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "X_train_trans = pipe.named_steps['pre'].transform(X_train)\n",
    "\n",
    "ohe_names = []\n",
    "if len(cat_features) > 0:\n",
    "    ohe_fitted = pipe.named_steps['pre'].named_transformers_['cat']\n",
    "    ohe_names = list(ohe_fitted.get_feature_names_out(cat_features))\n",
    "feature_names_final = ohe_names + num_features\n",
    "\n",
    "\n",
    "explainer = shap.TreeExplainer(pipe.named_steps['clf'])\n",
    "ns = min(5000, X_train_trans.shape[0])\n",
    "shap_values = explainer.shap_values(X_train_trans[:ns])\n",
    "\n",
    "\n",
    "mean_abs = np.abs(shap_values).mean(axis=0)\n",
    "shap_imp_rows = []\n",
    "if len(ohe_names) > 0:\n",
    "    for base in cat_features:\n",
    "        idxs = [i for i, fn in enumerate(ohe_names) if fn.startswith(f\"{base}_\")]\n",
    "        if idxs:\n",
    "            shap_imp_rows.append([base, float(mean_abs[idxs].sum())])\n",
    "for j, fn in enumerate(feature_names_final):\n",
    "    if fn in num_features:\n",
    "        shap_imp_rows.append([fn, float(mean_abs[j])])\n",
    "\n",
    "shap_imp_df = pd.DataFrame(shap_imp_rows, columns=['Feature', 'Mean|SHAP|']).sort_values('Mean|SHAP|', ascending=False)\n",
    "shap_imp_df['SHAP_Rank'] = np.arange(1, len(shap_imp_df) + 1)\n",
    "shap_imp_df.to_excel(OUTDIR / \"SHAP_Importance_Aggregated.xlsx\", index=False)\n",
    "shap_imp_df.to_csv(OUTDIR / \"SHAP_Importance_Aggregated.csv\", index=False)t\n",
    "shap_focus = shap_imp_df[shap_imp_df['Feature'].isin(features_of_interest)].copy()\n",
    "shap_focus.to_excel(OUTDIR / \"SHAP_Importance_FeaturesOfInterest.xlsx\", index=False)\n",
    "shap_focus.to_csv(OUTDIR / \"SHAP_Importance_FeaturesOfInterest.csv\", index=False)\n",
    "# ---------------------------\n",
    "merged_focus = (stat_focus[['Feature','Test','Statistic','EffectSize','EffectType','p_value','q_value']]\n",
    "                .merge(shap_focus[['Feature','Mean|SHAP|','SHAP_Rank']], on='Feature', how='outer'))\n",
    "merged_focus = merged_focus.sort_values(['q_value','SHAP_Rank'], na_position='last')\n",
    "merged_focus.to_excel(OUTDIR / \"Empirical_Eval_FeaturesOfInterest.xlsx\", index=False)\n",
    "merged_focus.to_csv(OUTDIR / \"Empirical_Eval_FeaturesOfInterest.csv\", index=False))\n",
    "plt.figure(figsize=(7, 5))\n",
    "topN = shap_imp_df.head(N_TOP_PLOT)['Feature'].tolist()\n",
    "plt.barh(topN[::-1], shap_imp_df.set_index('Feature').loc[topN, 'Mean|SHAP|'][::-1].values)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / f\"SHAP_SummaryBar_Top{N_TOP_PLOT}.png\", dpi=DPI)\n",
    "plt.close()\n",
    "shap.summary_plot(shap_values, X_train_trans[:ns], feature_names=feature_names_final, show=False, max_display=N_TOP_PLOT)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / f\"SHAP_Beeswarm_Top{N_TOP_PLOT}.png\", dpi=DPI, bbox_inches='tight')\n",
    "plt.close()\n",
    "dep_outdir = OUTDIR / \"dependence_plots\"\n",
    "dep_outdir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "dep_feats = ['TPT_per_Unit', 'LeadTime_Deviation', 'Weight_per_Unit', 'log_UnitQty',\n",
    "             'carrier_origin_risk', 'route_cum_late_rate', 'route_bb_mean',\n",
    "             'carrier_bb_mean', 'route_orders_last7d', 'route_roll10_Weight_q90',\n",
    "             'congestion_trend', 'Weight_vsCarrierMean', 'seq_pos_norm']\n",
    "\n",
    "for feat in dep_feats:\n",
    "    if feat not in num_features:\n",
    "        continue\n",
    "    try:\n",
    "        j = feature_names_final.index(feat)\n",
    "        shap.dependence_plot(ind=j, shap_values=shap_values, features=X_train_trans[:ns],\n",
    "                             feature_names=feature_names_final, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dep_outdir / f\"SHAP_Dependence_{feat}.png\", dpi=DPI, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not create dependence plot for {feat}: {e}\")\n",
    "\n",
    "print(f\"\\nSaved outputs in: {OUTDIR.resolve()}\")\n",
    "print(\"Tables:\\n - Empirical_Stats_Augmented.(xlsx/csv)\"\n",
    "      \"\\n - Empirical_Stats_FeaturesOfInterest.(xlsx/csv)\"\n",
    "      \"\\n - SHAP_Importance_Aggregated.(xlsx/csv)\"\n",
    "      \"\\n - SHAP_Importance_FeaturesOfInterest.(xlsx/csv)\"\n",
    "      \"\\n - Empirical_Eval_FeaturesOfInterest.(xlsx/csv)\")\n",
    "print(f\"Figures:\\n - SHAP_SummaryBar_Top{N_TOP_PLOT}.png\"\n",
    "      f\"\\n - SHAP_Beeswarm_Top{N_TOP_PLOT}.png\"\n",
    "      \"\\n - dependence_plots/*.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f0d287-2e53-4ec9-85f2-672f8e53cd72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda3)",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
