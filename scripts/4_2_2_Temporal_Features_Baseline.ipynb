{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73e6ea4-dcbe-463d-aa52-976c719bd53b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T02:33:05.687841Z",
     "iopub.status.busy": "2025-08-25T02:33:05.687841Z",
     "iopub.status.idle": "2025-08-25T02:35:29.666514Z",
     "shell.execute_reply": "2025-08-25T02:35:29.662237Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented features detected: ['adaptive_sla', 'congestion_flag', 'disruption_flag', 'time_drift', 'OrderMonth', 'OrderWeek', 'OrderWeekday', 'is_weekend', 'Weight_per_Unit', 'TPT_per_Unit', 'LeadTime_Deviation', 'Weight_vsCarrierMean', 'Unit_vsCarrierMean', 'Carrier_Origin', 'Plant_Destination', 'log_Weight', 'log_UnitQty', 'congestion_trend', 'sla_risk_trend', 'disruption_trend', 'time_drift_abs', 'carrier_origin_risk', 'route_cum_late_rate', 'route_bb_mean', 'carrier_cum_late_rate', 'carrier_bb_mean', 'seq_pos_norm', 'interarrival_idx_gap', 'route_roll10_TPT_mean', 'route_roll10_TPT_std', 'route_roll10_TPT_q90', 'route_roll10_LeadTime_Deviation_mean', 'route_roll10_LeadTime_Deviation_std', 'route_roll10_LeadTime_Deviation_q90', 'route_roll10_Weight_mean', 'route_roll10_Weight_std', 'route_roll10_Weight_q90', 'route_orders_last7d', 'route_orders_last14d', 'route_orders_last30d']\n",
      "Features of interest (present): ['Origin Port', 'Carrier', 'Plant Code', 'Destination Port', 'Plant_Destination', 'Unit quantity', 'Weight', 'TPT', 'TPT_per_Unit', 'LeadTime_Deviation', 'Weight_per_Unit', 'log_UnitQty', 'carrier_origin_risk', 'route_cum_late_rate', 'route_bb_mean', 'carrier_bb_mean', 'route_orders_last7d', 'route_roll10_Weight_q90', 'congestion_trend', 'Weight_vsCarrierMean', 'seq_pos_norm']\n",
      "\n",
      "Saved outputs in: C:\\Users\\ramak\\SupplyChain-DL-RL\\results\\4_2_2_Temporal_Features_Baseline\n",
      "Tables:\n",
      " - Empirical_Stats_Augmented.(xlsx/csv)\n",
      " - Empirical_Stats_FeaturesOfInterest.(xlsx/csv)\n",
      " - SHAP_Importance_Aggregated.(xlsx/csv)\n",
      " - SHAP_Importance_FeaturesOfInterest.(xlsx/csv)\n",
      " - Empirical_Eval_FeaturesOfInterest.(xlsx/csv)\n",
      "Figures:\n",
      " - SHAP_SummaryBar_Top20.png\n",
      " - SHAP_Beeswarm_Top20.png\n",
      " - dependence_plots/*.png\n"
     ]
    }
   ],
   "source": [
    "import sys, asyncio, os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency, pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import __version__ as sklver\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "if sys.platform.startswith(\"win\"):\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "\n",
    "DATA_FILENAME = \"Hybrid_Augmented_TSAFE_Features.xlsx\"\n",
    "def resolve_data_path(filename=DATA_FILENAME):\n",
    "    cwd = Path.cwd()\n",
    "    candidates = [\n",
    "        cwd / \"Data\" / filename,\n",
    "        cwd / \"data\" / filename,\n",
    "        cwd.parent / \"Data\" / filename,\n",
    "        cwd.parent / \"data\" / filename,\n",
    "        cwd / filename,\n",
    "        cwd.parent / filename,\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Could not find data file. Tried:\\n\" + \"\\n\".join(str(p) for p in candidates) + f\"\\nCWD={cwd}\")\n",
    "\n",
    "file_path = str(resolve_data_path())\n",
    "\n",
    "# Ensure results always go to repo root, not Scripts/\n",
    "ROOT = Path.cwd()\n",
    "if ROOT.name.lower() == \"scripts\":   # if running from Scripts folder\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "OUTDIR = ROOT / \"results\" / \"4_2_2_Temporal_Features_Baseline\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DPI = 600\n",
    "N_TOP_PLOT = 20\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "df = pd.read_excel(file_path)\n",
    "if \"Order Date\" in df.columns:\n",
    "    df[\"Order Date\"] = pd.to_datetime(df[\"Order Date\"])\n",
    "    df = df.sort_values([\"Order Date\"]).reset_index(drop=True)\n",
    "if \"Plant_Destination\" not in df.columns:\n",
    "    if {\"Plant Code\", \"Destination Port\"}.issubset(df.columns):\n",
    "        df[\"Plant_Destination\"] = df[\"Plant Code\"].astype(str) + \" | \" + df[\"Destination Port\"].astype(str)\n",
    "\n",
    "original_features = [\n",
    "    \"Order ID\",\"Order Date\",\"Origin Port\",\"Carrier\",\"TPT\",\"Service Level\",\n",
    "    \"Ship ahead day count\",\"Ship Late Day count\",\"Customer\",\"Product ID\",\n",
    "    \"Plant Code\",\"Destination Port\",\"Unit quantity\",\"Weight\"\n",
    "]\n",
    "all_features = df.columns.tolist()\n",
    "augmented_features = [c for c in all_features if c not in original_features]\n",
    "features_of_interest_cat = [\"Origin Port\",\"Carrier\",\"Plant Code\",\"Destination Port\",\"Plant_Destination\"]\n",
    "features_of_interest_num = [\n",
    "    \"Unit quantity\",\"Weight\",\"TPT\",\n",
    "    \"TPT_per_Unit\",\"LeadTime_Deviation\",\"Weight_per_Unit\",\n",
    "    \"log_UnitQty\",\"carrier_origin_risk\",\"route_cum_late_rate\",\n",
    "    \"route_bb_mean\",\"carrier_bb_mean\",\"route_orders_last7d\",\n",
    "    \"route_roll10_Weight_q90\",\"congestion_trend\",\"Weight_vsCarrierMean\",\n",
    "    \"seq_pos_norm\"\n",
    "]\n",
    "features_of_interest_cat = [c for c in features_of_interest_cat if c in df.columns]\n",
    "features_of_interest_num = [c for c in features_of_interest_num if c in df.columns]\n",
    "features_of_interest = features_of_interest_cat + features_of_interest_num\n",
    "\n",
    "print(\"Augmented features detected:\", augmented_features)\n",
    "print(\"Features of interest (present):\", features_of_interest)\n",
    "\n",
    "if \"Ship Late Day count\" not in df.columns:\n",
    "    raise ValueError(\"Missing 'Ship Late Day count' for target.\")\n",
    "y = (df[\"Ship Late Day count\"] > 0).astype(int)\n",
    "\n",
    "X_aug = df[augmented_features].copy() if augmented_features else pd.DataFrame(index=df.index)\n",
    "X_aug = X_aug.replace([np.inf, -np.inf], np.nan)\n",
    "for col in X_aug.select_dtypes(include=[np.number]).columns:\n",
    "    X_aug[col] = X_aug[col].fillna(X_aug[col].median())\n",
    "\n",
    "def cramers_v_from_chi2(chi2, n, k_rows, k_cols):\n",
    "    phi2 = chi2 / n\n",
    "    r, k = k_rows, k_cols\n",
    "    phi2corr = max(0, phi2 - (k-1)*(r-1)/(n-1))\n",
    "    rcorr = r - (r-1)**2/(n-1)\n",
    "    kcorr = k - (k-1)**2/(n-1)\n",
    "    denom = min(kcorr-1, rcorr-1)\n",
    "    return np.sqrt(phi2corr / denom) if denom > 0 else 0.0\n",
    "\n",
    "stat_rows = []\n",
    "for feat in augmented_features:\n",
    "    s = X_aug[feat]\n",
    "    try:\n",
    "        if s.dtype == \"object\" or s.nunique(dropna=True) <= 10:\n",
    "            tbl = pd.crosstab(s, y)\n",
    "            if tbl.shape[0] >= 2 and tbl.shape[1] >= 2:\n",
    "                chi2, p, dof, exp = chi2_contingency(tbl)\n",
    "                V = cramers_v_from_chi2(chi2, n=len(df), k_rows=tbl.shape[0], k_cols=tbl.shape[1])\n",
    "                stat_rows.append([feat, \"Chi-Square\", chi2, p, V, \"CramÃ©r_V\"])\n",
    "            else:\n",
    "                stat_rows.append([feat, \"Chi-Square\", np.nan, np.nan, np.nan, \"Insufficient categories for chi2\"])\n",
    "        else:\n",
    "            valid = s.dropna().index\n",
    "            if len(valid) > 2 and y.loc[valid].nunique() > 1:\n",
    "                r, p = pearsonr(s.loc[valid], y.loc[valid])\n",
    "                stat_rows.append([feat, \"Point-Biserial (Pearson)\", r, p, abs(r), \"|r_pb|\"])\n",
    "            else:\n",
    "                stat_rows.append([feat, \"Point-Biserial (Pearson)\", np.nan, np.nan, np.nan, \"Insufficient variance\"])\n",
    "    except Exception as e:\n",
    "        stat_rows.append([feat, \"ERROR\", np.nan, np.nan, np.nan, str(e)])\n",
    "\n",
    "stat_df = pd.DataFrame(stat_rows, columns=[\"Feature\",\"Test\",\"Statistic\",\"p_value\",\"EffectSize\",\"EffectType\"])\n",
    "mask_ok = stat_df[\"p_value\"].notna()\n",
    "if mask_ok.any():\n",
    "    stat_df.loc[mask_ok, \"q_value\"] = multipletests(stat_df.loc[mask_ok, \"p_value\"].values, method=\"fdr_bh\")[1]\n",
    "else:\n",
    "    stat_df[\"q_value\"] = np.nan\n",
    "stat_df = stat_df.sort_values([\"q_value\",\"p_value\"], na_position=\"last\")\n",
    "stat_df.to_excel(OUTDIR / \"Empirical_Stats_Augmented.xlsx\", index=False)\n",
    "stat_df.to_csv(OUTDIR / \"Empirical_Stats_Augmented.csv\", index=False)\n",
    "stat_focus = stat_df[stat_df[\"Feature\"].isin(features_of_interest)].copy()\n",
    "stat_focus.to_excel(OUTDIR / \"Empirical_Stats_FeaturesOfInterest.xlsx\", index=False)\n",
    "stat_focus.to_csv(OUTDIR / \"Empirical_Stats_FeaturesOfInterest.csv\", index=False)\n",
    "\n",
    "cat_features = features_of_interest_cat\n",
    "num_features = features_of_interest_num\n",
    "X_base = df[cat_features + num_features].copy()\n",
    "if \"Order Date\" in df.columns:\n",
    "    cutoff = df[\"Order Date\"].quantile(0.8)\n",
    "    train_idx = df[\"Order Date\"] <= cutoff\n",
    "    test_idx  = df[\"Order Date\"] >  cutoff\n",
    "else:\n",
    "    train_idx, test_idx = train_test_split(df.index, test_size=0.2, stratify=y, random_state=SEED)\n",
    "X_train = X_base.loc[train_idx]\n",
    "X_test  = X_base.loc[test_idx]\n",
    "y_train = y.loc[train_idx]\n",
    "y_test  = y.loc[test_idx]\n",
    "\n",
    "major, minor = map(int, sklver.split(\".\")[:2])\n",
    "if (major, minor) >= (1, 2):\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "else:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "pre = ColumnTransformer([(\"cat\", ohe, cat_features),(\"num\", \"passthrough\", num_features)],remainder=\"drop\")\n",
    "xgb = XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.05,subsample=0.9, colsample_bytree=0.9,eval_metric=\"logloss\", random_state=SEED)\n",
    "pipe = Pipeline([(\"pre\", pre), (\"clf\", xgb)])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "X_train_trans = pipe.named_steps[\"pre\"].transform(X_train)\n",
    "ohe_names = []\n",
    "if len(cat_features) > 0:\n",
    "    ohe_fitted = pipe.named_steps[\"pre\"].named_transformers_[\"cat\"]\n",
    "    ohe_names = list(ohe_fitted.get_feature_names_out(cat_features))\n",
    "feature_names_final = ohe_names + num_features\n",
    "\n",
    "explainer = shap.TreeExplainer(pipe.named_steps[\"clf\"])\n",
    "ns = min(5000, X_train_trans.shape[0])\n",
    "shap_values = explainer.shap_values(X_train_trans[:ns])\n",
    "mean_abs = np.abs(shap_values).mean(axis=0)\n",
    "shap_imp_rows = []\n",
    "if len(ohe_names) > 0:\n",
    "    for base in cat_features:\n",
    "        idxs = [i for i, fn in enumerate(ohe_names) if fn.startswith(f\"{base}_\")]\n",
    "        if idxs:\n",
    "            shap_imp_rows.append([base, float(mean_abs[idxs].sum())])\n",
    "for j, fn in enumerate(feature_names_final):\n",
    "    if fn in num_features:\n",
    "        shap_imp_rows.append([fn, float(mean_abs[j])])\n",
    "shap_imp_df = pd.DataFrame(shap_imp_rows, columns=[\"Feature\",\"Mean|SHAP|\"]).sort_values(\"Mean|SHAP|\", ascending=False)\n",
    "shap_imp_df[\"SHAP_Rank\"] = np.arange(1, len(shap_imp_df) + 1)\n",
    "shap_imp_df.to_excel(OUTDIR / \"SHAP_Importance_Aggregated.xlsx\", index=False)\n",
    "shap_imp_df.to_csv(OUTDIR / \"SHAP_Importance_Aggregated.csv\", index=False)\n",
    "shap_focus = shap_imp_df[shap_imp_df[\"Feature\"].isin(features_of_interest)].copy()\n",
    "shap_focus.to_excel(OUTDIR / \"SHAP_Importance_FeaturesOfInterest.xlsx\", index=False)\n",
    "shap_focus.to_csv(OUTDIR / \"SHAP_Importance_FeaturesOfInterest.csv\", index=False)\n",
    "\n",
    "merged_focus = (stat_focus[[\"Feature\",\"Test\",\"Statistic\",\"EffectSize\",\"EffectType\",\"p_value\",\"q_value\"]].merge(shap_focus[[\"Feature\",\"Mean|SHAP|\",\"SHAP_Rank\"]], on=\"Feature\", how=\"outer\")).sort_values([\"q_value\",\"SHAP_Rank\"], na_position=\"last\")\n",
    "merged_focus.to_excel(OUTDIR / \"Empirical_Eval_FeaturesOfInterest.xlsx\", index=False)\n",
    "merged_focus.to_csv(OUTDIR / \"Empirical_Eval_FeaturesOfInterest.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "topN = shap_imp_df.head(N_TOP_PLOT)[\"Feature\"].tolist()\n",
    "plt.barh(topN[::-1], shap_imp_df.set_index(\"Feature\").loc[topN, \"Mean|SHAP|\"][::-1].values)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / f\"SHAP_SummaryBar_Top{N_TOP_PLOT}.png\", dpi=DPI)\n",
    "plt.close()\n",
    "shap.summary_plot(shap_values, X_train_trans[:ns], feature_names=feature_names_final, show=False, max_display=N_TOP_PLOT)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / f\"SHAP_Beeswarm_Top{N_TOP_PLOT}.png\", dpi=DPI, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "dep_outdir = OUTDIR / \"dependence_plots\"\n",
    "dep_outdir.mkdir(exist_ok=True, parents=True)\n",
    "dep_feats = ['TPT_per_Unit','LeadTime_Deviation','Weight_per_Unit','log_UnitQty','carrier_origin_risk','route_cum_late_rate','route_bb_mean','carrier_bb_mean','route_orders_last7d','route_roll10_Weight_q90','congestion_trend','Weight_vsCarrierMean','seq_pos_norm']\n",
    "for feat in dep_feats:\n",
    "    if feat not in features_of_interest_num:\n",
    "        continue\n",
    "    try:\n",
    "        j = feature_names_final.index(feat)\n",
    "        shap.dependence_plot(ind=j, shap_values=shap_values, features=X_train_trans[:ns],feature_names=feature_names_final, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dep_outdir / f\"SHAP_Dependence_{feat}.png\", dpi=DPI, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not create dependence plot for {feat}: {e}\")\n",
    "\n",
    "print(f\"\\nSaved outputs in: {OUTDIR.resolve()}\")\n",
    "print(\"Tables:\\n - Empirical_Stats_Augmented.(xlsx/csv)\\n - Empirical_Stats_FeaturesOfInterest.(xlsx/csv)\\n - SHAP_Importance_Aggregated.(xlsx/csv)\\n - SHAP_Importance_FeaturesOfInterest.(xlsx/csv)\\n - Empirical_Eval_FeaturesOfInterest.(xlsx/csv)\")\n",
    "print(f\"Figures:\\n - SHAP_SummaryBar_Top{N_TOP_PLOT}.png\\n - SHAP_Beeswarm_Top{N_TOP_PLOT}.png\\n - dependence_plots/*.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda3)",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
